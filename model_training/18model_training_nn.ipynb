{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from protein_encoding import PC_6, read_fasta\n",
    "from model_tools import split, show_train_history, metric_array\n",
    "from model_edit_18 import train_model\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gpu\n",
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data\n",
    "# PC6 encoding\n",
    "AVP_data = PC_6('../data/2024_pos_trainval_6db_2880.fasta', length=50)\n",
    "non_AVP_data = PC_6('../data/combined_16995_negativeset.fasta', length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict to np array\n",
    "AVP_array= np.array(list(AVP_data.values()))\n",
    "non_AVP_array = np.array(list(non_AVP_data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "non_AVP_array = random.sample(list(non_AVP_array), len(AVP_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature & labels (0:non_AVP, 1:AVP)\n",
    "features = np.concatenate((non_AVP_array,AVP_array),axis=0)\n",
    "labels = np.hstack((np.repeat(0, len(non_AVP_array)),np.repeat(1, len(AVP_array))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train validation split\n",
    "train_data, val_data, train_labels, val_labels = split(features, labels , save = False, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "train_data, train_labels = shuffle(train_data, train_labels, random_state=0)\n",
    "val_data, val_labels = shuffle(val_data, val_labels, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 3.5634 - accuracy: 0.4955\n",
      "Epoch 00001: val_loss improved from inf to 1.54357, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 8s 53ms/step - loss: 3.5568 - accuracy: 0.4951 - val_loss: 1.5436 - val_accuracy: 0.4665 - lr: 2.0000e-04\n",
      "Epoch 2/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 2.6293 - accuracy: 0.5100\n",
      "Epoch 00002: val_loss improved from 1.54357 to 1.52545, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 2.6278 - accuracy: 0.5102 - val_loss: 1.5254 - val_accuracy: 0.5541 - lr: 2.0000e-04\n",
      "Epoch 3/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 2.0459 - accuracy: 0.5690\n",
      "Epoch 00003: val_loss improved from 1.52545 to 1.47325, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 2.0437 - accuracy: 0.5698 - val_loss: 1.4732 - val_accuracy: 0.6637 - lr: 2.0000e-04\n",
      "Epoch 4/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.7309 - accuracy: 0.6306\n",
      "Epoch 00004: val_loss improved from 1.47325 to 1.43253, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 1.7300 - accuracy: 0.6300 - val_loss: 1.4325 - val_accuracy: 0.7139 - lr: 2.0000e-04\n",
      "Epoch 5/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.6210 - accuracy: 0.6532\n",
      "Epoch 00005: val_loss improved from 1.43253 to 1.40220, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 1.6211 - accuracy: 0.6536 - val_loss: 1.4022 - val_accuracy: 0.7049 - lr: 2.0000e-04\n",
      "Epoch 6/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.5128 - accuracy: 0.6894\n",
      "Epoch 00006: val_loss improved from 1.40220 to 1.39275, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 1.5138 - accuracy: 0.6890 - val_loss: 1.3927 - val_accuracy: 0.6894 - lr: 2.0000e-04\n",
      "Epoch 7/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.4773 - accuracy: 0.7006\n",
      "Epoch 00007: val_loss improved from 1.39275 to 1.37849, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 1.4739 - accuracy: 0.7016 - val_loss: 1.3785 - val_accuracy: 0.6997 - lr: 2.0000e-04\n",
      "Epoch 8/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.4354 - accuracy: 0.7122\n",
      "Epoch 00008: val_loss improved from 1.37849 to 1.36273, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 1.4354 - accuracy: 0.7122 - val_loss: 1.3627 - val_accuracy: 0.6997 - lr: 2.0000e-04\n",
      "Epoch 9/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.3932 - accuracy: 0.7256\n",
      "Epoch 00009: val_loss improved from 1.36273 to 1.34240, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 1.3932 - accuracy: 0.7256 - val_loss: 1.3424 - val_accuracy: 0.7281 - lr: 2.0000e-04\n",
      "Epoch 10/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3744 - accuracy: 0.7275\n",
      "Epoch 00010: val_loss improved from 1.34240 to 1.32954, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 1.3739 - accuracy: 0.7277 - val_loss: 1.3295 - val_accuracy: 0.7294 - lr: 2.0000e-04\n",
      "Epoch 11/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3401 - accuracy: 0.7399\n",
      "Epoch 00011: val_loss improved from 1.32954 to 1.30925, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 1.3394 - accuracy: 0.7399 - val_loss: 1.3093 - val_accuracy: 0.7410 - lr: 2.0000e-04\n",
      "Epoch 12/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.3192 - accuracy: 0.7462\n",
      "Epoch 00012: val_loss improved from 1.30925 to 1.29080, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 1.3177 - accuracy: 0.7469 - val_loss: 1.2908 - val_accuracy: 0.7539 - lr: 2.0000e-04\n",
      "Epoch 13/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2941 - accuracy: 0.7481\n",
      "Epoch 00013: val_loss improved from 1.29080 to 1.27407, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 1.2957 - accuracy: 0.7475 - val_loss: 1.2741 - val_accuracy: 0.7655 - lr: 2.0000e-04\n",
      "Epoch 14/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2782 - accuracy: 0.7528\n",
      "Epoch 00014: val_loss improved from 1.27407 to 1.25905, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 1.2780 - accuracy: 0.7527 - val_loss: 1.2591 - val_accuracy: 0.7668 - lr: 2.0000e-04\n",
      "Epoch 15/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.2625 - accuracy: 0.7526\n",
      "Epoch 00015: val_loss improved from 1.25905 to 1.24448, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 1.2623 - accuracy: 0.7522 - val_loss: 1.2445 - val_accuracy: 0.7706 - lr: 2.0000e-04\n",
      "Epoch 16/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.2303 - accuracy: 0.7668\n",
      "Epoch 00016: val_loss improved from 1.24448 to 1.23293, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 1.2303 - accuracy: 0.7668 - val_loss: 1.2329 - val_accuracy: 0.7745 - lr: 2.0000e-04\n",
      "Epoch 17/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.2081 - accuracy: 0.7733\n",
      "Epoch 00017: val_loss improved from 1.23293 to 1.21610, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 1.2081 - accuracy: 0.7733 - val_loss: 1.2161 - val_accuracy: 0.7796 - lr: 2.0000e-04\n",
      "Epoch 18/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.1892 - accuracy: 0.7752\n",
      "Epoch 00018: val_loss improved from 1.21610 to 1.19758, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 1.1892 - accuracy: 0.7752 - val_loss: 1.1976 - val_accuracy: 0.7964 - lr: 2.0000e-04\n",
      "Epoch 19/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1804 - accuracy: 0.7693\n",
      "Epoch 00019: val_loss improved from 1.19758 to 1.18255, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 1.1808 - accuracy: 0.7694 - val_loss: 1.1825 - val_accuracy: 0.7964 - lr: 2.0000e-04\n",
      "Epoch 20/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1614 - accuracy: 0.7728\n",
      "Epoch 00020: val_loss improved from 1.18255 to 1.16452, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 1.1610 - accuracy: 0.7727 - val_loss: 1.1645 - val_accuracy: 0.7977 - lr: 2.0000e-04\n",
      "Epoch 21/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1298 - accuracy: 0.7865\n",
      "Epoch 00021: val_loss improved from 1.16452 to 1.14787, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 1.1310 - accuracy: 0.7858 - val_loss: 1.1479 - val_accuracy: 0.8015 - lr: 2.0000e-04\n",
      "Epoch 22/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.1126 - accuracy: 0.7900\n",
      "Epoch 00022: val_loss improved from 1.14787 to 1.12911, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 1.1140 - accuracy: 0.7891 - val_loss: 1.1291 - val_accuracy: 0.8041 - lr: 2.0000e-04\n",
      "Epoch 23/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0910 - accuracy: 0.7917\n",
      "Epoch 00023: val_loss improved from 1.12911 to 1.11332, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 1.0908 - accuracy: 0.7916 - val_loss: 1.1133 - val_accuracy: 0.8144 - lr: 2.0000e-04\n",
      "Epoch 24/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0731 - accuracy: 0.7974\n",
      "Epoch 00024: val_loss improved from 1.11332 to 1.09770, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 1.0723 - accuracy: 0.7976 - val_loss: 1.0977 - val_accuracy: 0.8080 - lr: 2.0000e-04\n",
      "Epoch 25/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0602 - accuracy: 0.7910\n",
      "Epoch 00025: val_loss improved from 1.09770 to 1.08174, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 1.0582 - accuracy: 0.7922 - val_loss: 1.0817 - val_accuracy: 0.8080 - lr: 2.0000e-04\n",
      "Epoch 26/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 1.0291 - accuracy: 0.8058\n",
      "Epoch 00026: val_loss improved from 1.08174 to 1.06027, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 1.0291 - accuracy: 0.8054 - val_loss: 1.0603 - val_accuracy: 0.8157 - lr: 2.0000e-04\n",
      "Epoch 27/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 1.0152 - accuracy: 0.8050\n",
      "Epoch 00027: val_loss improved from 1.06027 to 1.04182, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 1.0152 - accuracy: 0.8050 - val_loss: 1.0418 - val_accuracy: 0.8183 - lr: 2.0000e-04\n",
      "Epoch 28/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9976 - accuracy: 0.8070\n",
      "Epoch 00028: val_loss improved from 1.04182 to 1.02661, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.9965 - accuracy: 0.8075 - val_loss: 1.0266 - val_accuracy: 0.8183 - lr: 2.0000e-04\n",
      "Epoch 29/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9758 - accuracy: 0.8130\n",
      "Epoch 00029: val_loss improved from 1.02661 to 1.00911, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.9756 - accuracy: 0.8130 - val_loss: 1.0091 - val_accuracy: 0.8144 - lr: 2.0000e-04\n",
      "Epoch 30/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9584 - accuracy: 0.8155\n",
      "Epoch 00030: val_loss improved from 1.00911 to 0.98932, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.9578 - accuracy: 0.8157 - val_loss: 0.9893 - val_accuracy: 0.8299 - lr: 2.0000e-04\n",
      "Epoch 31/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9434 - accuracy: 0.8146\n",
      "Epoch 00031: val_loss improved from 0.98932 to 0.97241, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.9434 - accuracy: 0.8146 - val_loss: 0.9724 - val_accuracy: 0.8247 - lr: 2.0000e-04\n",
      "Epoch 32/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9186 - accuracy: 0.8138\n",
      "Epoch 00032: val_loss improved from 0.97241 to 0.95418, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.9186 - accuracy: 0.8140 - val_loss: 0.9542 - val_accuracy: 0.8312 - lr: 2.0000e-04\n",
      "Epoch 33/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.9071 - accuracy: 0.8178\n",
      "Epoch 00033: val_loss improved from 0.95418 to 0.93711, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.9075 - accuracy: 0.8177 - val_loss: 0.9371 - val_accuracy: 0.8260 - lr: 2.0000e-04\n",
      "Epoch 34/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.8770 - accuracy: 0.8270\n",
      "Epoch 00034: val_loss improved from 0.93711 to 0.92067, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.8770 - accuracy: 0.8270 - val_loss: 0.9207 - val_accuracy: 0.8312 - lr: 2.0000e-04\n",
      "Epoch 35/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8642 - accuracy: 0.8235\n",
      "Epoch 00035: val_loss improved from 0.92067 to 0.90513, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.8652 - accuracy: 0.8232 - val_loss: 0.9051 - val_accuracy: 0.8286 - lr: 2.0000e-04\n",
      "Epoch 36/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8411 - accuracy: 0.8293\n",
      "Epoch 00036: val_loss improved from 0.90513 to 0.88294, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.8409 - accuracy: 0.8289 - val_loss: 0.8829 - val_accuracy: 0.8273 - lr: 2.0000e-04\n",
      "Epoch 37/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8197 - accuracy: 0.8280\n",
      "Epoch 00037: val_loss improved from 0.88294 to 0.86725, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.8202 - accuracy: 0.8276 - val_loss: 0.8672 - val_accuracy: 0.8299 - lr: 2.0000e-04\n",
      "Epoch 38/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.8079 - accuracy: 0.8280\n",
      "Epoch 00038: val_loss improved from 0.86725 to 0.85068, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.8076 - accuracy: 0.8279 - val_loss: 0.8507 - val_accuracy: 0.8286 - lr: 2.0000e-04\n",
      "Epoch 39/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7994 - accuracy: 0.8307\n",
      "Epoch 00039: val_loss improved from 0.85068 to 0.83451, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.7983 - accuracy: 0.8313 - val_loss: 0.8345 - val_accuracy: 0.8312 - lr: 2.0000e-04\n",
      "Epoch 40/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7774 - accuracy: 0.8314\n",
      "Epoch 00040: val_loss improved from 0.83451 to 0.81415, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.7767 - accuracy: 0.8318 - val_loss: 0.8141 - val_accuracy: 0.8338 - lr: 2.0000e-04\n",
      "Epoch 41/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7600 - accuracy: 0.8341\n",
      "Epoch 00041: val_loss improved from 0.81415 to 0.79670, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.7605 - accuracy: 0.8342 - val_loss: 0.7967 - val_accuracy: 0.8402 - lr: 2.0000e-04\n",
      "Epoch 42/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7449 - accuracy: 0.8345\n",
      "Epoch 00042: val_loss improved from 0.79670 to 0.78451, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.7448 - accuracy: 0.8348 - val_loss: 0.7845 - val_accuracy: 0.8531 - lr: 2.0000e-04\n",
      "Epoch 43/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7331 - accuracy: 0.8372\n",
      "Epoch 00043: val_loss improved from 0.78451 to 0.77715, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.7336 - accuracy: 0.8375 - val_loss: 0.7772 - val_accuracy: 0.8415 - lr: 2.0000e-04\n",
      "Epoch 44/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.7183 - accuracy: 0.8358\n",
      "Epoch 00044: val_loss improved from 0.77715 to 0.75955, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.7192 - accuracy: 0.8356 - val_loss: 0.7595 - val_accuracy: 0.8415 - lr: 2.0000e-04\n",
      "Epoch 45/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6974 - accuracy: 0.8444\n",
      "Epoch 00045: val_loss improved from 0.75955 to 0.74321, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.6974 - accuracy: 0.8444 - val_loss: 0.7432 - val_accuracy: 0.8557 - lr: 2.0000e-04\n",
      "Epoch 46/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6855 - accuracy: 0.8391\n",
      "Epoch 00046: val_loss improved from 0.74321 to 0.73453, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.6846 - accuracy: 0.8398 - val_loss: 0.7345 - val_accuracy: 0.8454 - lr: 2.0000e-04\n",
      "Epoch 47/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6615 - accuracy: 0.8477\n",
      "Epoch 00047: val_loss improved from 0.73453 to 0.72208, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.6638 - accuracy: 0.8465 - val_loss: 0.7221 - val_accuracy: 0.8415 - lr: 2.0000e-04\n",
      "Epoch 48/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6551 - accuracy: 0.8480\n",
      "Epoch 00048: val_loss improved from 0.72208 to 0.70651, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.6551 - accuracy: 0.8480 - val_loss: 0.7065 - val_accuracy: 0.8570 - lr: 2.0000e-04\n",
      "Epoch 49/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6401 - accuracy: 0.8474\n",
      "Epoch 00049: val_loss improved from 0.70651 to 0.69276, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.6401 - accuracy: 0.8477 - val_loss: 0.6928 - val_accuracy: 0.8531 - lr: 2.0000e-04\n",
      "Epoch 50/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6267 - accuracy: 0.8471\n",
      "Epoch 00050: val_loss improved from 0.69276 to 0.67633, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.6267 - accuracy: 0.8471 - val_loss: 0.6763 - val_accuracy: 0.8608 - lr: 2.0000e-04\n",
      "Epoch 51/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.6101 - accuracy: 0.8561\n",
      "Epoch 00051: val_loss improved from 0.67633 to 0.67001, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.6096 - accuracy: 0.8565 - val_loss: 0.6700 - val_accuracy: 0.8505 - lr: 2.0000e-04\n",
      "Epoch 52/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.6080 - accuracy: 0.8472\n",
      "Epoch 00052: val_loss improved from 0.67001 to 0.66032, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.6080 - accuracy: 0.8472 - val_loss: 0.6603 - val_accuracy: 0.8544 - lr: 2.0000e-04\n",
      "Epoch 53/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5853 - accuracy: 0.8584\n",
      "Epoch 00053: val_loss improved from 0.66032 to 0.65240, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.5866 - accuracy: 0.8580 - val_loss: 0.6524 - val_accuracy: 0.8570 - lr: 2.0000e-04\n",
      "Epoch 54/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5761 - accuracy: 0.8564\n",
      "Epoch 00054: val_loss improved from 0.65240 to 0.63029, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.5761 - accuracy: 0.8564 - val_loss: 0.6303 - val_accuracy: 0.8595 - lr: 2.0000e-04\n",
      "Epoch 55/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5681 - accuracy: 0.8574\n",
      "Epoch 00055: val_loss improved from 0.63029 to 0.62236, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.5685 - accuracy: 0.8573 - val_loss: 0.6224 - val_accuracy: 0.8724 - lr: 2.0000e-04\n",
      "Epoch 56/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.5548 - accuracy: 0.8587\n",
      "Epoch 00056: val_loss improved from 0.62236 to 0.60565, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.5548 - accuracy: 0.8587 - val_loss: 0.6056 - val_accuracy: 0.8647 - lr: 2.0000e-04\n",
      "Epoch 57/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5461 - accuracy: 0.8584\n",
      "Epoch 00057: val_loss did not improve from 0.60565\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.5466 - accuracy: 0.8581 - val_loss: 0.6128 - val_accuracy: 0.8557 - lr: 2.0000e-04\n",
      "Epoch 58/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5385 - accuracy: 0.8606\n",
      "Epoch 00058: val_loss improved from 0.60565 to 0.59103, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.5392 - accuracy: 0.8597 - val_loss: 0.5910 - val_accuracy: 0.8621 - lr: 2.0000e-04\n",
      "Epoch 59/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5209 - accuracy: 0.8638\n",
      "Epoch 00059: val_loss improved from 0.59103 to 0.57462, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.5197 - accuracy: 0.8644 - val_loss: 0.5746 - val_accuracy: 0.8698 - lr: 2.0000e-04\n",
      "Epoch 60/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.5094 - accuracy: 0.8688\n",
      "Epoch 00060: val_loss improved from 0.57462 to 0.57082, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.5105 - accuracy: 0.8684 - val_loss: 0.5708 - val_accuracy: 0.8660 - lr: 2.0000e-04\n",
      "Epoch 61/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4978 - accuracy: 0.8739\n",
      "Epoch 00061: val_loss improved from 0.57082 to 0.56879, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.4985 - accuracy: 0.8733 - val_loss: 0.5688 - val_accuracy: 0.8660 - lr: 2.0000e-04\n",
      "Epoch 62/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4933 - accuracy: 0.8682\n",
      "Epoch 00062: val_loss improved from 0.56879 to 0.56137, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.4933 - accuracy: 0.8682 - val_loss: 0.5614 - val_accuracy: 0.8531 - lr: 2.0000e-04\n",
      "Epoch 63/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4884 - accuracy: 0.8675\n",
      "Epoch 00063: val_loss improved from 0.56137 to 0.54768, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.4872 - accuracy: 0.8682 - val_loss: 0.5477 - val_accuracy: 0.8634 - lr: 2.0000e-04\n",
      "Epoch 64/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4724 - accuracy: 0.8699\n",
      "Epoch 00064: val_loss did not improve from 0.54768\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.4731 - accuracy: 0.8694 - val_loss: 0.5482 - val_accuracy: 0.8608 - lr: 2.0000e-04\n",
      "Epoch 65/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4697 - accuracy: 0.8683\n",
      "Epoch 00065: val_loss improved from 0.54768 to 0.53800, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.4689 - accuracy: 0.8687 - val_loss: 0.5380 - val_accuracy: 0.8673 - lr: 2.0000e-04\n",
      "Epoch 66/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4680 - accuracy: 0.8677\n",
      "Epoch 00066: val_loss improved from 0.53800 to 0.52356, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.4689 - accuracy: 0.8667 - val_loss: 0.5236 - val_accuracy: 0.8737 - lr: 2.0000e-04\n",
      "Epoch 67/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4595 - accuracy: 0.8732\n",
      "Epoch 00067: val_loss did not improve from 0.52356\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.4595 - accuracy: 0.8732 - val_loss: 0.5354 - val_accuracy: 0.8724 - lr: 2.0000e-04\n",
      "Epoch 68/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.4578 - accuracy: 0.8689\n",
      "Epoch 00068: val_loss improved from 0.52356 to 0.51367, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.4578 - accuracy: 0.8689 - val_loss: 0.5137 - val_accuracy: 0.8763 - lr: 2.0000e-04\n",
      "Epoch 69/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4406 - accuracy: 0.8777\n",
      "Epoch 00069: val_loss improved from 0.51367 to 0.50985, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.4411 - accuracy: 0.8776 - val_loss: 0.5099 - val_accuracy: 0.8621 - lr: 2.0000e-04\n",
      "Epoch 70/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4362 - accuracy: 0.8754\n",
      "Epoch 00070: val_loss improved from 0.50985 to 0.49187, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.4354 - accuracy: 0.8759 - val_loss: 0.4919 - val_accuracy: 0.8737 - lr: 2.0000e-04\n",
      "Epoch 71/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4280 - accuracy: 0.8816\n",
      "Epoch 00071: val_loss did not improve from 0.49187\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.4286 - accuracy: 0.8818 - val_loss: 0.4920 - val_accuracy: 0.8827 - lr: 2.0000e-04\n",
      "Epoch 72/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4264 - accuracy: 0.8775\n",
      "Epoch 00072: val_loss improved from 0.49187 to 0.48850, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.4258 - accuracy: 0.8778 - val_loss: 0.4885 - val_accuracy: 0.8802 - lr: 2.0000e-04\n",
      "Epoch 73/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4198 - accuracy: 0.8813\n",
      "Epoch 00073: val_loss did not improve from 0.48850\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.4198 - accuracy: 0.8815 - val_loss: 0.4915 - val_accuracy: 0.8789 - lr: 2.0000e-04\n",
      "Epoch 74/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4137 - accuracy: 0.8816\n",
      "Epoch 00074: val_loss improved from 0.48850 to 0.48193, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.4139 - accuracy: 0.8815 - val_loss: 0.4819 - val_accuracy: 0.8840 - lr: 2.0000e-04\n",
      "Epoch 75/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.4119 - accuracy: 0.8762\n",
      "Epoch 00075: val_loss improved from 0.48193 to 0.47391, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.4119 - accuracy: 0.8763 - val_loss: 0.4739 - val_accuracy: 0.8686 - lr: 2.0000e-04\n",
      "Epoch 76/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3973 - accuracy: 0.8858\n",
      "Epoch 00076: val_loss improved from 0.47391 to 0.46777, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.3973 - accuracy: 0.8858 - val_loss: 0.4678 - val_accuracy: 0.8853 - lr: 2.0000e-04\n",
      "Epoch 77/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3925 - accuracy: 0.8886\n",
      "Epoch 00077: val_loss improved from 0.46777 to 0.46521, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.3919 - accuracy: 0.8891 - val_loss: 0.4652 - val_accuracy: 0.8750 - lr: 2.0000e-04\n",
      "Epoch 78/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3827 - accuracy: 0.8898\n",
      "Epoch 00078: val_loss improved from 0.46521 to 0.45580, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.3827 - accuracy: 0.8898 - val_loss: 0.4558 - val_accuracy: 0.8866 - lr: 2.0000e-04\n",
      "Epoch 79/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3820 - accuracy: 0.8929\n",
      "Epoch 00079: val_loss improved from 0.45580 to 0.45386, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.3825 - accuracy: 0.8927 - val_loss: 0.4539 - val_accuracy: 0.8802 - lr: 2.0000e-04\n",
      "Epoch 80/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3785 - accuracy: 0.8914\n",
      "Epoch 00080: val_loss did not improve from 0.45386\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.3792 - accuracy: 0.8914 - val_loss: 0.4702 - val_accuracy: 0.8711 - lr: 2.0000e-04\n",
      "Epoch 81/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3869 - accuracy: 0.8844\n",
      "Epoch 00081: val_loss improved from 0.45386 to 0.44575, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.3869 - accuracy: 0.8844 - val_loss: 0.4458 - val_accuracy: 0.8943 - lr: 2.0000e-04\n",
      "Epoch 82/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3794 - accuracy: 0.8901\n",
      "Epoch 00082: val_loss improved from 0.44575 to 0.44154, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.3786 - accuracy: 0.8905 - val_loss: 0.4415 - val_accuracy: 0.8956 - lr: 2.0000e-04\n",
      "Epoch 83/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3752 - accuracy: 0.8897\n",
      "Epoch 00083: val_loss improved from 0.44154 to 0.43670, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.3752 - accuracy: 0.8897 - val_loss: 0.4367 - val_accuracy: 0.8789 - lr: 2.0000e-04\n",
      "Epoch 84/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3670 - accuracy: 0.8945\n",
      "Epoch 00084: val_loss improved from 0.43670 to 0.43107, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.3670 - accuracy: 0.8945 - val_loss: 0.4311 - val_accuracy: 0.8956 - lr: 2.0000e-04\n",
      "Epoch 85/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3664 - accuracy: 0.8899\n",
      "Epoch 00085: val_loss did not improve from 0.43107\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.3668 - accuracy: 0.8895 - val_loss: 0.4346 - val_accuracy: 0.8866 - lr: 2.0000e-04\n",
      "Epoch 86/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3539 - accuracy: 0.8951\n",
      "Epoch 00086: val_loss improved from 0.43107 to 0.42849, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.3540 - accuracy: 0.8950 - val_loss: 0.4285 - val_accuracy: 0.8930 - lr: 2.0000e-04\n",
      "Epoch 87/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3628 - accuracy: 0.8928\n",
      "Epoch 00087: val_loss did not improve from 0.42849\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.3614 - accuracy: 0.8935 - val_loss: 0.4296 - val_accuracy: 0.8686 - lr: 2.0000e-04\n",
      "Epoch 88/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3468 - accuracy: 0.8968\n",
      "Epoch 00088: val_loss did not improve from 0.42849\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.3462 - accuracy: 0.8972 - val_loss: 0.4291 - val_accuracy: 0.8840 - lr: 2.0000e-04\n",
      "Epoch 89/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3465 - accuracy: 0.8987\n",
      "Epoch 00089: val_loss improved from 0.42849 to 0.41473, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.3465 - accuracy: 0.8987 - val_loss: 0.4147 - val_accuracy: 0.8879 - lr: 2.0000e-04\n",
      "Epoch 90/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3377 - accuracy: 0.9010\n",
      "Epoch 00090: val_loss did not improve from 0.41473\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.3377 - accuracy: 0.9008 - val_loss: 0.4216 - val_accuracy: 0.8879 - lr: 2.0000e-04\n",
      "Epoch 91/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3438 - accuracy: 0.8958\n",
      "Epoch 00091: val_loss did not improve from 0.41473\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.3431 - accuracy: 0.8962 - val_loss: 0.4220 - val_accuracy: 0.8918 - lr: 2.0000e-04\n",
      "Epoch 92/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3360 - accuracy: 0.8971\n",
      "Epoch 00092: val_loss did not improve from 0.41473\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.3360 - accuracy: 0.8971 - val_loss: 0.4149 - val_accuracy: 0.8853 - lr: 2.0000e-04\n",
      "Epoch 93/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3375 - accuracy: 0.8977\n",
      "Epoch 00093: val_loss improved from 0.41473 to 0.40373, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.3375 - accuracy: 0.8977 - val_loss: 0.4037 - val_accuracy: 0.8840 - lr: 2.0000e-04\n",
      "Epoch 94/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3307 - accuracy: 0.8980\n",
      "Epoch 00094: val_loss improved from 0.40373 to 0.39458, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.3304 - accuracy: 0.8980 - val_loss: 0.3946 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 95/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3261 - accuracy: 0.9017\n",
      "Epoch 00095: val_loss did not improve from 0.39458\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.3261 - accuracy: 0.9017 - val_loss: 0.4117 - val_accuracy: 0.8737 - lr: 2.0000e-04\n",
      "Epoch 96/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3173 - accuracy: 0.9068\n",
      "Epoch 00096: val_loss improved from 0.39458 to 0.38586, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.3181 - accuracy: 0.9066 - val_loss: 0.3859 - val_accuracy: 0.8956 - lr: 2.0000e-04\n",
      "Epoch 97/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3226 - accuracy: 0.9078\n",
      "Epoch 00097: val_loss did not improve from 0.38586\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.3225 - accuracy: 0.9077 - val_loss: 0.3873 - val_accuracy: 0.8956 - lr: 2.0000e-04\n",
      "Epoch 98/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.9080\n",
      "Epoch 00098: val_loss did not improve from 0.38586\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.3153 - accuracy: 0.9080 - val_loss: 0.3931 - val_accuracy: 0.8930 - lr: 2.0000e-04\n",
      "Epoch 99/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3173 - accuracy: 0.9038\n",
      "Epoch 00099: val_loss did not improve from 0.38586\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.3165 - accuracy: 0.9040 - val_loss: 0.3905 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 100/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3208 - accuracy: 0.9025\n",
      "Epoch 00100: val_loss did not improve from 0.38586\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.3209 - accuracy: 0.9027 - val_loss: 0.3903 - val_accuracy: 0.8943 - lr: 2.0000e-04\n",
      "Epoch 101/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.9034\n",
      "Epoch 00101: val_loss did not improve from 0.38586\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.3127 - accuracy: 0.9034 - val_loss: 0.3883 - val_accuracy: 0.8930 - lr: 2.0000e-04\n",
      "Epoch 102/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.3091 - accuracy: 0.9080\n",
      "Epoch 00102: val_loss improved from 0.38586 to 0.38500, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.3095 - accuracy: 0.9077 - val_loss: 0.3850 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 103/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2994 - accuracy: 0.9155\n",
      "Epoch 00103: val_loss improved from 0.38500 to 0.37168, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.3016 - accuracy: 0.9147 - val_loss: 0.3717 - val_accuracy: 0.8943 - lr: 2.0000e-04\n",
      "Epoch 104/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2979 - accuracy: 0.9091\n",
      "Epoch 00104: val_loss did not improve from 0.37168\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2968 - accuracy: 0.9097 - val_loss: 0.3736 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 105/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.9143\n",
      "Epoch 00105: val_loss did not improve from 0.37168\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2941 - accuracy: 0.9143 - val_loss: 0.3812 - val_accuracy: 0.8918 - lr: 2.0000e-04\n",
      "Epoch 106/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2998 - accuracy: 0.9096\n",
      "Epoch 00106: val_loss improved from 0.37168 to 0.37085, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.3001 - accuracy: 0.9094 - val_loss: 0.3709 - val_accuracy: 0.9124 - lr: 2.0000e-04\n",
      "Epoch 107/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2943 - accuracy: 0.9144\n",
      "Epoch 00107: val_loss improved from 0.37085 to 0.36795, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.2943 - accuracy: 0.9144 - val_loss: 0.3679 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 108/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.9117\n",
      "Epoch 00108: val_loss improved from 0.36795 to 0.35806, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2943 - accuracy: 0.9119 - val_loss: 0.3581 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 109/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2968 - accuracy: 0.9125\n",
      "Epoch 00109: val_loss did not improve from 0.35806\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2963 - accuracy: 0.9130 - val_loss: 0.3735 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 110/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2974 - accuracy: 0.9067\n",
      "Epoch 00110: val_loss did not improve from 0.35806\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2977 - accuracy: 0.9067 - val_loss: 0.3724 - val_accuracy: 0.8789 - lr: 2.0000e-04\n",
      "Epoch 111/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2839 - accuracy: 0.9183\n",
      "Epoch 00111: val_loss did not improve from 0.35806\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2839 - accuracy: 0.9183 - val_loss: 0.3758 - val_accuracy: 0.8840 - lr: 2.0000e-04\n",
      "Epoch 112/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2840 - accuracy: 0.9146\n",
      "Epoch 00112: val_loss did not improve from 0.35806\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2854 - accuracy: 0.9142 - val_loss: 0.3588 - val_accuracy: 0.8982 - lr: 2.0000e-04\n",
      "Epoch 113/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2784 - accuracy: 0.9172\n",
      "Epoch 00113: val_loss did not improve from 0.35806\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2784 - accuracy: 0.9172 - val_loss: 0.3629 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 114/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2752 - accuracy: 0.9172\n",
      "Epoch 00114: val_loss did not improve from 0.35806\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2761 - accuracy: 0.9167 - val_loss: 0.3657 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 115/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2767 - accuracy: 0.9184\n",
      "Epoch 00115: val_loss improved from 0.35806 to 0.34169, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2764 - accuracy: 0.9187 - val_loss: 0.3417 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 116/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2753 - accuracy: 0.9177\n",
      "Epoch 00116: val_loss did not improve from 0.34169\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2753 - accuracy: 0.9177 - val_loss: 0.3522 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 117/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2751 - accuracy: 0.9138\n",
      "Epoch 00117: val_loss did not improve from 0.34169\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2748 - accuracy: 0.9140 - val_loss: 0.3515 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 118/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2756 - accuracy: 0.9177\n",
      "Epoch 00118: val_loss did not improve from 0.34169\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2749 - accuracy: 0.9179 - val_loss: 0.3549 - val_accuracy: 0.9124 - lr: 2.0000e-04\n",
      "Epoch 119/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2713 - accuracy: 0.9206\n",
      "Epoch 00119: val_loss did not improve from 0.34169\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2709 - accuracy: 0.9210 - val_loss: 0.3504 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 120/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2790 - accuracy: 0.9143\n",
      "Epoch 00120: val_loss improved from 0.34169 to 0.34066, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.2789 - accuracy: 0.9144 - val_loss: 0.3407 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 121/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2735 - accuracy: 0.9204\n",
      "Epoch 00121: val_loss did not improve from 0.34066\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2725 - accuracy: 0.9210 - val_loss: 0.3407 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 122/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2682 - accuracy: 0.9212\n",
      "Epoch 00122: val_loss did not improve from 0.34066\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2681 - accuracy: 0.9215 - val_loss: 0.3427 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 123/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2625 - accuracy: 0.9200\n",
      "Epoch 00123: val_loss did not improve from 0.34066\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2621 - accuracy: 0.9205 - val_loss: 0.3474 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 124/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2618 - accuracy: 0.9226\n",
      "Epoch 00124: val_loss improved from 0.34066 to 0.33317, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.2618 - accuracy: 0.9226 - val_loss: 0.3332 - val_accuracy: 0.9124 - lr: 2.0000e-04\n",
      "Epoch 125/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2656 - accuracy: 0.9216\n",
      "Epoch 00125: val_loss did not improve from 0.33317\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2654 - accuracy: 0.9219 - val_loss: 0.3578 - val_accuracy: 0.8956 - lr: 2.0000e-04\n",
      "Epoch 126/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2597 - accuracy: 0.9238\n",
      "Epoch 00126: val_loss did not improve from 0.33317\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2594 - accuracy: 0.9242 - val_loss: 0.3357 - val_accuracy: 0.9137 - lr: 2.0000e-04\n",
      "Epoch 127/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2596 - accuracy: 0.9212\n",
      "Epoch 00127: val_loss did not improve from 0.33317\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2588 - accuracy: 0.9219 - val_loss: 0.3388 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 128/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2518 - accuracy: 0.9246\n",
      "Epoch 00128: val_loss did not improve from 0.33317\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2510 - accuracy: 0.9251 - val_loss: 0.3351 - val_accuracy: 0.8930 - lr: 2.0000e-04\n",
      "Epoch 129/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2502 - accuracy: 0.9248\n",
      "Epoch 00129: val_loss improved from 0.33317 to 0.33270, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2511 - accuracy: 0.9242 - val_loss: 0.3327 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 130/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2528 - accuracy: 0.9223\n",
      "Epoch 00130: val_loss did not improve from 0.33270\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2528 - accuracy: 0.9223 - val_loss: 0.3407 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 131/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2591 - accuracy: 0.9206\n",
      "Epoch 00131: val_loss did not improve from 0.33270\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2591 - accuracy: 0.9206 - val_loss: 0.3337 - val_accuracy: 0.9008 - lr: 2.0000e-04\n",
      "Epoch 132/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2565 - accuracy: 0.9258\n",
      "Epoch 00132: val_loss improved from 0.33270 to 0.33088, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.2555 - accuracy: 0.9263 - val_loss: 0.3309 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 133/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.9223\n",
      "Epoch 00133: val_loss did not improve from 0.33088\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.2536 - accuracy: 0.9223 - val_loss: 0.3311 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 134/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2446 - accuracy: 0.9255\n",
      "Epoch 00134: val_loss improved from 0.33088 to 0.33008, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.2454 - accuracy: 0.9251 - val_loss: 0.3301 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 135/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2495 - accuracy: 0.9264\n",
      "Epoch 00135: val_loss improved from 0.33008 to 0.32211, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2498 - accuracy: 0.9259 - val_loss: 0.3221 - val_accuracy: 0.9008 - lr: 2.0000e-04\n",
      "Epoch 136/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2490 - accuracy: 0.9252\n",
      "Epoch 00136: val_loss did not improve from 0.32211\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2486 - accuracy: 0.9258 - val_loss: 0.3332 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 137/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2454 - accuracy: 0.9313\n",
      "Epoch 00137: val_loss improved from 0.32211 to 0.32193, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2461 - accuracy: 0.9306 - val_loss: 0.3219 - val_accuracy: 0.8930 - lr: 2.0000e-04\n",
      "Epoch 138/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2449 - accuracy: 0.9293\n",
      "Epoch 00138: val_loss improved from 0.32193 to 0.32008, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2441 - accuracy: 0.9299 - val_loss: 0.3201 - val_accuracy: 0.8982 - lr: 2.0000e-04\n",
      "Epoch 139/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2425 - accuracy: 0.9283\n",
      "Epoch 00139: val_loss did not improve from 0.32008\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2424 - accuracy: 0.9283 - val_loss: 0.3309 - val_accuracy: 0.8982 - lr: 2.0000e-04\n",
      "Epoch 140/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2512 - accuracy: 0.9235\n",
      "Epoch 00140: val_loss did not improve from 0.32008\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2505 - accuracy: 0.9238 - val_loss: 0.3256 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 141/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2361 - accuracy: 0.9296\n",
      "Epoch 00141: val_loss did not improve from 0.32008\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2372 - accuracy: 0.9292 - val_loss: 0.3286 - val_accuracy: 0.9149 - lr: 2.0000e-04\n",
      "Epoch 142/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2364 - accuracy: 0.9303\n",
      "Epoch 00142: val_loss did not improve from 0.32008\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2368 - accuracy: 0.9302 - val_loss: 0.3260 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 143/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2336 - accuracy: 0.9303\n",
      "Epoch 00143: val_loss did not improve from 0.32008\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2335 - accuracy: 0.9302 - val_loss: 0.3288 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 144/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2462 - accuracy: 0.9286\n",
      "Epoch 00144: val_loss improved from 0.32008 to 0.31759, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.2458 - accuracy: 0.9288 - val_loss: 0.3176 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 145/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2307 - accuracy: 0.9333\n",
      "Epoch 00145: val_loss improved from 0.31759 to 0.31223, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.2304 - accuracy: 0.9336 - val_loss: 0.3122 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 146/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2351 - accuracy: 0.9333\n",
      "Epoch 00146: val_loss did not improve from 0.31223\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2354 - accuracy: 0.9332 - val_loss: 0.3192 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 147/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2363 - accuracy: 0.9310\n",
      "Epoch 00147: val_loss did not improve from 0.31223\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2354 - accuracy: 0.9312 - val_loss: 0.3341 - val_accuracy: 0.8943 - lr: 2.0000e-04\n",
      "Epoch 148/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2295 - accuracy: 0.9346\n",
      "Epoch 00148: val_loss did not improve from 0.31223\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2289 - accuracy: 0.9349 - val_loss: 0.3229 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 149/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2295 - accuracy: 0.9349\n",
      "Epoch 00149: val_loss did not improve from 0.31223\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2292 - accuracy: 0.9351 - val_loss: 0.3269 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 150/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2348 - accuracy: 0.9332\n",
      "Epoch 00150: val_loss improved from 0.31223 to 0.30940, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.2342 - accuracy: 0.9334 - val_loss: 0.3094 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 151/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2291 - accuracy: 0.9332\n",
      "Epoch 00151: val_loss did not improve from 0.30940\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2291 - accuracy: 0.9332 - val_loss: 0.3346 - val_accuracy: 0.8853 - lr: 2.0000e-04\n",
      "Epoch 152/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2269 - accuracy: 0.9364\n",
      "Epoch 00152: val_loss improved from 0.30940 to 0.30444, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2276 - accuracy: 0.9359 - val_loss: 0.3044 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 153/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2359 - accuracy: 0.9306\n",
      "Epoch 00153: val_loss did not improve from 0.30444\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2366 - accuracy: 0.9302 - val_loss: 0.3223 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 154/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2241 - accuracy: 0.9341\n",
      "Epoch 00154: val_loss did not improve from 0.30444\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2231 - accuracy: 0.9347 - val_loss: 0.3069 - val_accuracy: 0.9149 - lr: 2.0000e-04\n",
      "Epoch 155/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2213 - accuracy: 0.9367\n",
      "Epoch 00155: val_loss did not improve from 0.30444\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2210 - accuracy: 0.9367 - val_loss: 0.3154 - val_accuracy: 0.9137 - lr: 2.0000e-04\n",
      "Epoch 156/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2327 - accuracy: 0.9306\n",
      "Epoch 00156: val_loss did not improve from 0.30444\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2327 - accuracy: 0.9306 - val_loss: 0.3231 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 157/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2248 - accuracy: 0.9341\n",
      "Epoch 00157: val_loss did not improve from 0.30444\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2256 - accuracy: 0.9335 - val_loss: 0.3094 - val_accuracy: 0.9149 - lr: 2.0000e-04\n",
      "Epoch 158/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2268 - accuracy: 0.9358\n",
      "Epoch 00158: val_loss did not improve from 0.30444\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2280 - accuracy: 0.9352 - val_loss: 0.3342 - val_accuracy: 0.9124 - lr: 2.0000e-04\n",
      "Epoch 159/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2225 - accuracy: 0.9343\n",
      "Epoch 00159: val_loss did not improve from 0.30444\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2219 - accuracy: 0.9347 - val_loss: 0.3097 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 160/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2288 - accuracy: 0.9344\n",
      "Epoch 00160: val_loss improved from 0.30444 to 0.29980, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.2288 - accuracy: 0.9344 - val_loss: 0.2998 - val_accuracy: 0.9201 - lr: 2.0000e-04\n",
      "Epoch 161/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2225 - accuracy: 0.9348\n",
      "Epoch 00161: val_loss did not improve from 0.29980\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2226 - accuracy: 0.9348 - val_loss: 0.3196 - val_accuracy: 0.8982 - lr: 2.0000e-04\n",
      "Epoch 162/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2275 - accuracy: 0.9314\n",
      "Epoch 00162: val_loss did not improve from 0.29980\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2275 - accuracy: 0.9314 - val_loss: 0.3012 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 163/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2270 - accuracy: 0.9322\n",
      "Epoch 00163: val_loss did not improve from 0.29980\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2268 - accuracy: 0.9321 - val_loss: 0.3051 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 164/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2266 - accuracy: 0.9320\n",
      "Epoch 00164: val_loss did not improve from 0.29980\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2271 - accuracy: 0.9319 - val_loss: 0.3082 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 165/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2152 - accuracy: 0.9407\n",
      "Epoch 00165: val_loss did not improve from 0.29980\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2152 - accuracy: 0.9407 - val_loss: 0.3191 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 166/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2224 - accuracy: 0.9328\n",
      "Epoch 00166: val_loss improved from 0.29980 to 0.29843, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.2223 - accuracy: 0.9326 - val_loss: 0.2984 - val_accuracy: 0.9124 - lr: 2.0000e-04\n",
      "Epoch 167/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2206 - accuracy: 0.9345\n",
      "Epoch 00167: val_loss did not improve from 0.29843\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2206 - accuracy: 0.9345 - val_loss: 0.3105 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 168/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2181 - accuracy: 0.9394\n",
      "Epoch 00168: val_loss did not improve from 0.29843\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2177 - accuracy: 0.9395 - val_loss: 0.3073 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 169/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2137 - accuracy: 0.9375\n",
      "Epoch 00169: val_loss did not improve from 0.29843\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2135 - accuracy: 0.9379 - val_loss: 0.3047 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 170/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2146 - accuracy: 0.9387\n",
      "Epoch 00170: val_loss did not improve from 0.29843\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2154 - accuracy: 0.9385 - val_loss: 0.3189 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 171/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2155 - accuracy: 0.9380\n",
      "Epoch 00171: val_loss did not improve from 0.29843\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2149 - accuracy: 0.9382 - val_loss: 0.3149 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 172/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2165 - accuracy: 0.9352\n",
      "Epoch 00172: val_loss improved from 0.29843 to 0.29576, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.2169 - accuracy: 0.9352 - val_loss: 0.2958 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 173/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2111 - accuracy: 0.9358\n",
      "Epoch 00173: val_loss did not improve from 0.29576\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2111 - accuracy: 0.9358 - val_loss: 0.3007 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 174/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2121 - accuracy: 0.9381\n",
      "Epoch 00174: val_loss did not improve from 0.29576\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2119 - accuracy: 0.9384 - val_loss: 0.3039 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 175/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2102 - accuracy: 0.9390\n",
      "Epoch 00175: val_loss did not improve from 0.29576\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2103 - accuracy: 0.9390 - val_loss: 0.3078 - val_accuracy: 0.8982 - lr: 2.0000e-04\n",
      "Epoch 176/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2232 - accuracy: 0.9324\n",
      "Epoch 00176: val_loss did not improve from 0.29576\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2232 - accuracy: 0.9324 - val_loss: 0.3040 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 177/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2093 - accuracy: 0.9407\n",
      "Epoch 00177: val_loss did not improve from 0.29576\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2093 - accuracy: 0.9407 - val_loss: 0.2976 - val_accuracy: 0.9149 - lr: 2.0000e-04\n",
      "Epoch 178/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2109 - accuracy: 0.9370\n",
      "Epoch 00178: val_loss did not improve from 0.29576\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2116 - accuracy: 0.9365 - val_loss: 0.3033 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 179/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2104 - accuracy: 0.9403\n",
      "Epoch 00179: val_loss did not improve from 0.29576\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2108 - accuracy: 0.9401 - val_loss: 0.3008 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 180/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2185 - accuracy: 0.9382\n",
      "Epoch 00180: val_loss did not improve from 0.29576\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2185 - accuracy: 0.9382 - val_loss: 0.3093 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 181/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2080 - accuracy: 0.9435\n",
      "Epoch 00181: val_loss improved from 0.29576 to 0.29409, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2092 - accuracy: 0.9433 - val_loss: 0.2941 - val_accuracy: 0.9008 - lr: 2.0000e-04\n",
      "Epoch 182/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2118 - accuracy: 0.9397\n",
      "Epoch 00182: val_loss did not improve from 0.29409\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2114 - accuracy: 0.9398 - val_loss: 0.3090 - val_accuracy: 0.8930 - lr: 2.0000e-04\n",
      "Epoch 183/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2115 - accuracy: 0.9414\n",
      "Epoch 00183: val_loss improved from 0.29409 to 0.29253, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.2117 - accuracy: 0.9415 - val_loss: 0.2925 - val_accuracy: 0.9124 - lr: 2.0000e-04\n",
      "Epoch 184/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2031 - accuracy: 0.9409\n",
      "Epoch 00184: val_loss did not improve from 0.29253\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2034 - accuracy: 0.9405 - val_loss: 0.3019 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 185/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2035 - accuracy: 0.9410\n",
      "Epoch 00185: val_loss did not improve from 0.29253\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2034 - accuracy: 0.9411 - val_loss: 0.3038 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 186/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1976 - accuracy: 0.9442\n",
      "Epoch 00186: val_loss improved from 0.29253 to 0.29005, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.1987 - accuracy: 0.9440 - val_loss: 0.2900 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 187/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2075 - accuracy: 0.9392\n",
      "Epoch 00187: val_loss did not improve from 0.29005\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2075 - accuracy: 0.9392 - val_loss: 0.3002 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 188/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2062 - accuracy: 0.9434\n",
      "Epoch 00188: val_loss did not improve from 0.29005\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2062 - accuracy: 0.9434 - val_loss: 0.2979 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 189/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2080 - accuracy: 0.9399\n",
      "Epoch 00189: val_loss improved from 0.29005 to 0.28952, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.2074 - accuracy: 0.9401 - val_loss: 0.2895 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 190/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2041 - accuracy: 0.9400\n",
      "Epoch 00190: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2048 - accuracy: 0.9397 - val_loss: 0.3049 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 191/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2133 - accuracy: 0.9394\n",
      "Epoch 00191: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2151 - accuracy: 0.9388 - val_loss: 0.2993 - val_accuracy: 0.8982 - lr: 2.0000e-04\n",
      "Epoch 192/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2040 - accuracy: 0.9397\n",
      "Epoch 00192: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2035 - accuracy: 0.9401 - val_loss: 0.3152 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 193/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2057 - accuracy: 0.9404\n",
      "Epoch 00193: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2057 - accuracy: 0.9404 - val_loss: 0.2993 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 194/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.9425\n",
      "Epoch 00194: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2031 - accuracy: 0.9425 - val_loss: 0.2964 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 195/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1993 - accuracy: 0.9430\n",
      "Epoch 00195: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1999 - accuracy: 0.9428 - val_loss: 0.2997 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 196/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2056 - accuracy: 0.9432\n",
      "Epoch 00196: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2062 - accuracy: 0.9428 - val_loss: 0.3114 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 197/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2131 - accuracy: 0.9391\n",
      "Epoch 00197: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2131 - accuracy: 0.9391 - val_loss: 0.3132 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 198/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2023 - accuracy: 0.9414\n",
      "Epoch 00198: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.2030 - accuracy: 0.9414 - val_loss: 0.3188 - val_accuracy: 0.8853 - lr: 2.0000e-04\n",
      "Epoch 199/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2049 - accuracy: 0.9388\n",
      "Epoch 00199: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.2049 - accuracy: 0.9388 - val_loss: 0.3062 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 200/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2023 - accuracy: 0.9433\n",
      "Epoch 00200: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2021 - accuracy: 0.9434 - val_loss: 0.3004 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 201/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2036 - accuracy: 0.9461\n",
      "Epoch 00201: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2028 - accuracy: 0.9464 - val_loss: 0.3091 - val_accuracy: 0.8956 - lr: 2.0000e-04\n",
      "Epoch 202/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1967 - accuracy: 0.9441\n",
      "Epoch 00202: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1971 - accuracy: 0.9438 - val_loss: 0.2947 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 203/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1981 - accuracy: 0.9445\n",
      "Epoch 00203: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1981 - accuracy: 0.9445 - val_loss: 0.3014 - val_accuracy: 0.9124 - lr: 2.0000e-04\n",
      "Epoch 204/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.2028 - accuracy: 0.9443\n",
      "Epoch 00204: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2028 - accuracy: 0.9443 - val_loss: 0.2996 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 205/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1934 - accuracy: 0.9420\n",
      "Epoch 00205: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1929 - accuracy: 0.9422 - val_loss: 0.3183 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 206/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1958 - accuracy: 0.9423\n",
      "Epoch 00206: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1956 - accuracy: 0.9422 - val_loss: 0.2995 - val_accuracy: 0.8943 - lr: 2.0000e-04\n",
      "Epoch 207/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1930 - accuracy: 0.9475\n",
      "Epoch 00207: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1934 - accuracy: 0.9475 - val_loss: 0.3107 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 208/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1948 - accuracy: 0.9442\n",
      "Epoch 00208: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1953 - accuracy: 0.9441 - val_loss: 0.2937 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 209/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1991 - accuracy: 0.9432\n",
      "Epoch 00209: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1991 - accuracy: 0.9431 - val_loss: 0.3265 - val_accuracy: 0.8930 - lr: 2.0000e-04\n",
      "Epoch 210/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2055 - accuracy: 0.9404\n",
      "Epoch 00210: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.2054 - accuracy: 0.9402 - val_loss: 0.3153 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 211/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1965 - accuracy: 0.9433\n",
      "Epoch 00211: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1956 - accuracy: 0.9438 - val_loss: 0.2984 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 212/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1926 - accuracy: 0.9455\n",
      "Epoch 00212: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1927 - accuracy: 0.9455 - val_loss: 0.2924 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 213/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1858 - accuracy: 0.9503\n",
      "Epoch 00213: val_loss did not improve from 0.28952\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1858 - accuracy: 0.9503 - val_loss: 0.2906 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 214/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1903 - accuracy: 0.9517\n",
      "Epoch 00214: val_loss improved from 0.28952 to 0.28921, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.1903 - accuracy: 0.9517 - val_loss: 0.2892 - val_accuracy: 0.9124 - lr: 2.0000e-04\n",
      "Epoch 215/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1874 - accuracy: 0.9501\n",
      "Epoch 00215: val_loss did not improve from 0.28921\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1872 - accuracy: 0.9501 - val_loss: 0.3006 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 216/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1926 - accuracy: 0.9461\n",
      "Epoch 00216: val_loss improved from 0.28921 to 0.28403, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.1926 - accuracy: 0.9461 - val_loss: 0.2840 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 217/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1846 - accuracy: 0.9501\n",
      "Epoch 00217: val_loss did not improve from 0.28403\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1848 - accuracy: 0.9503 - val_loss: 0.2936 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 218/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1838 - accuracy: 0.9499\n",
      "Epoch 00218: val_loss did not improve from 0.28403\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1831 - accuracy: 0.9503 - val_loss: 0.2851 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 219/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1932 - accuracy: 0.9451\n",
      "Epoch 00219: val_loss did not improve from 0.28403\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1944 - accuracy: 0.9447 - val_loss: 0.2975 - val_accuracy: 0.9008 - lr: 2.0000e-04\n",
      "Epoch 220/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1936 - accuracy: 0.9490\n",
      "Epoch 00220: val_loss did not improve from 0.28403\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1928 - accuracy: 0.9494 - val_loss: 0.2842 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 221/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1905 - accuracy: 0.9484\n",
      "Epoch 00221: val_loss did not improve from 0.28403\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1905 - accuracy: 0.9484 - val_loss: 0.3001 - val_accuracy: 0.9008 - lr: 2.0000e-04\n",
      "Epoch 222/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1875 - accuracy: 0.9486\n",
      "Epoch 00222: val_loss did not improve from 0.28403\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1877 - accuracy: 0.9487 - val_loss: 0.2969 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 223/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1851 - accuracy: 0.9471\n",
      "Epoch 00223: val_loss did not improve from 0.28403\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1851 - accuracy: 0.9471 - val_loss: 0.2910 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 224/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1926 - accuracy: 0.9462\n",
      "Epoch 00224: val_loss did not improve from 0.28403\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1919 - accuracy: 0.9465 - val_loss: 0.2914 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 225/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1920 - accuracy: 0.9446\n",
      "Epoch 00225: val_loss improved from 0.28403 to 0.28277, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1914 - accuracy: 0.9451 - val_loss: 0.2828 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 226/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1901 - accuracy: 0.9475\n",
      "Epoch 00226: val_loss did not improve from 0.28277\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1919 - accuracy: 0.9474 - val_loss: 0.2905 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 227/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1837 - accuracy: 0.9497\n",
      "Epoch 00227: val_loss did not improve from 0.28277\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1842 - accuracy: 0.9496 - val_loss: 0.2855 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 228/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1845 - accuracy: 0.9507\n",
      "Epoch 00228: val_loss did not improve from 0.28277\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1850 - accuracy: 0.9510 - val_loss: 0.3213 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 229/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1915 - accuracy: 0.9481\n",
      "Epoch 00229: val_loss did not improve from 0.28277\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1914 - accuracy: 0.9480 - val_loss: 0.2910 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 230/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.2013 - accuracy: 0.9419\n",
      "Epoch 00230: val_loss improved from 0.28277 to 0.27786, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.2016 - accuracy: 0.9418 - val_loss: 0.2779 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 231/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1830 - accuracy: 0.9503\n",
      "Epoch 00231: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1827 - accuracy: 0.9506 - val_loss: 0.2833 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 232/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1910 - accuracy: 0.9478\n",
      "Epoch 00232: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1907 - accuracy: 0.9480 - val_loss: 0.3193 - val_accuracy: 0.8866 - lr: 2.0000e-04\n",
      "Epoch 233/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1863 - accuracy: 0.9487\n",
      "Epoch 00233: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1863 - accuracy: 0.9487 - val_loss: 0.2893 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 234/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.9523\n",
      "Epoch 00234: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1823 - accuracy: 0.9523 - val_loss: 0.2823 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 235/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1822 - accuracy: 0.9510\n",
      "Epoch 00235: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.1823 - accuracy: 0.9513 - val_loss: 0.2927 - val_accuracy: 0.8956 - lr: 2.0000e-04\n",
      "Epoch 236/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1790 - accuracy: 0.9529\n",
      "Epoch 00236: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1790 - accuracy: 0.9529 - val_loss: 0.3098 - val_accuracy: 0.8918 - lr: 2.0000e-04\n",
      "Epoch 237/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1862 - accuracy: 0.9483\n",
      "Epoch 00237: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1871 - accuracy: 0.9480 - val_loss: 0.3066 - val_accuracy: 0.9008 - lr: 2.0000e-04\n",
      "Epoch 238/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1937 - accuracy: 0.9462\n",
      "Epoch 00238: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1932 - accuracy: 0.9465 - val_loss: 0.2907 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 239/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1909 - accuracy: 0.9470\n",
      "Epoch 00239: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1909 - accuracy: 0.9470 - val_loss: 0.2797 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 240/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1835 - accuracy: 0.9477\n",
      "Epoch 00240: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1851 - accuracy: 0.9470 - val_loss: 0.2853 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 241/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1775 - accuracy: 0.9548\n",
      "Epoch 00241: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1777 - accuracy: 0.9546 - val_loss: 0.2940 - val_accuracy: 0.8943 - lr: 2.0000e-04\n",
      "Epoch 242/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1757 - accuracy: 0.9524\n",
      "Epoch 00242: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1757 - accuracy: 0.9524 - val_loss: 0.2866 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 243/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1781 - accuracy: 0.9535\n",
      "Epoch 00243: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1788 - accuracy: 0.9530 - val_loss: 0.2965 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 244/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1880 - accuracy: 0.9474\n",
      "Epoch 00244: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1887 - accuracy: 0.9471 - val_loss: 0.2978 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 245/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1828 - accuracy: 0.9501\n",
      "Epoch 00245: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1825 - accuracy: 0.9503 - val_loss: 0.2869 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 246/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1902 - accuracy: 0.9484\n",
      "Epoch 00246: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1895 - accuracy: 0.9487 - val_loss: 0.2900 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 247/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1797 - accuracy: 0.9512\n",
      "Epoch 00247: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1804 - accuracy: 0.9511 - val_loss: 0.2910 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 248/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1841 - accuracy: 0.9504\n",
      "Epoch 00248: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1841 - accuracy: 0.9504 - val_loss: 0.2976 - val_accuracy: 0.9149 - lr: 2.0000e-04\n",
      "Epoch 249/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1892 - accuracy: 0.9490\n",
      "Epoch 00249: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1898 - accuracy: 0.9490 - val_loss: 0.3230 - val_accuracy: 0.8776 - lr: 2.0000e-04\n",
      "Epoch 250/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1830 - accuracy: 0.9493\n",
      "Epoch 00250: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1830 - accuracy: 0.9493 - val_loss: 0.3148 - val_accuracy: 0.8956 - lr: 2.0000e-04\n",
      "Epoch 251/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1838 - accuracy: 0.9509\n",
      "Epoch 00251: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1845 - accuracy: 0.9504 - val_loss: 0.2917 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 252/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1840 - accuracy: 0.9504\n",
      "Epoch 00252: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1841 - accuracy: 0.9501 - val_loss: 0.3082 - val_accuracy: 0.8918 - lr: 2.0000e-04\n",
      "Epoch 253/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1724 - accuracy: 0.9536\n",
      "Epoch 00253: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1724 - accuracy: 0.9534 - val_loss: 0.2853 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 254/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1831 - accuracy: 0.9501\n",
      "Epoch 00254: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1823 - accuracy: 0.9506 - val_loss: 0.2854 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 255/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1788 - accuracy: 0.9488\n",
      "Epoch 00255: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1788 - accuracy: 0.9488 - val_loss: 0.2965 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 256/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1885 - accuracy: 0.9483\n",
      "Epoch 00256: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1876 - accuracy: 0.9487 - val_loss: 0.2847 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 257/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1745 - accuracy: 0.9541\n",
      "Epoch 00257: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1743 - accuracy: 0.9543 - val_loss: 0.2840 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 258/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1772 - accuracy: 0.9522\n",
      "Epoch 00258: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1767 - accuracy: 0.9526 - val_loss: 0.3034 - val_accuracy: 0.8918 - lr: 2.0000e-04\n",
      "Epoch 259/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1874 - accuracy: 0.9481\n",
      "Epoch 00259: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1874 - accuracy: 0.9481 - val_loss: 0.2833 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 260/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1802 - accuracy: 0.9506\n",
      "Epoch 00260: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1802 - accuracy: 0.9506 - val_loss: 0.2860 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 261/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1870 - accuracy: 0.9500\n",
      "Epoch 00261: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1867 - accuracy: 0.9501 - val_loss: 0.2969 - val_accuracy: 0.8918 - lr: 2.0000e-04\n",
      "Epoch 262/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1815 - accuracy: 0.9512\n",
      "Epoch 00262: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1814 - accuracy: 0.9514 - val_loss: 0.2937 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 263/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1831 - accuracy: 0.9513\n",
      "Epoch 00263: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1828 - accuracy: 0.9514 - val_loss: 0.2945 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 264/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1745 - accuracy: 0.9529\n",
      "Epoch 00264: val_loss did not improve from 0.27786\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1742 - accuracy: 0.9531 - val_loss: 0.2822 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 265/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1713 - accuracy: 0.9547\n",
      "Epoch 00265: val_loss improved from 0.27786 to 0.27415, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.1713 - accuracy: 0.9547 - val_loss: 0.2742 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 266/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1748 - accuracy: 0.9541\n",
      "Epoch 00266: val_loss did not improve from 0.27415\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1752 - accuracy: 0.9536 - val_loss: 0.3008 - val_accuracy: 0.9008 - lr: 2.0000e-04\n",
      "Epoch 267/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1731 - accuracy: 0.9543\n",
      "Epoch 00267: val_loss did not improve from 0.27415\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1741 - accuracy: 0.9539 - val_loss: 0.2826 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 268/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1767 - accuracy: 0.9517\n",
      "Epoch 00268: val_loss did not improve from 0.27415\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1774 - accuracy: 0.9517 - val_loss: 0.2774 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 269/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1822 - accuracy: 0.9490\n",
      "Epoch 00269: val_loss did not improve from 0.27415\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.1822 - accuracy: 0.9490 - val_loss: 0.2901 - val_accuracy: 0.8982 - lr: 2.0000e-04\n",
      "Epoch 270/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1658 - accuracy: 0.9599\n",
      "Epoch 00270: val_loss did not improve from 0.27415\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1663 - accuracy: 0.9593 - val_loss: 0.2856 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 271/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1809 - accuracy: 0.9523\n",
      "Epoch 00271: val_loss did not improve from 0.27415\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1804 - accuracy: 0.9526 - val_loss: 0.2819 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 272/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1845 - accuracy: 0.9500\n",
      "Epoch 00272: val_loss did not improve from 0.27415\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1852 - accuracy: 0.9497 - val_loss: 0.2823 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 273/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1795 - accuracy: 0.9518\n",
      "Epoch 00273: val_loss did not improve from 0.27415\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1795 - accuracy: 0.9518 - val_loss: 0.3061 - val_accuracy: 0.8918 - lr: 2.0000e-04\n",
      "Epoch 274/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1792 - accuracy: 0.9524\n",
      "Epoch 00274: val_loss improved from 0.27415 to 0.27361, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 45ms/step - loss: 0.1792 - accuracy: 0.9524 - val_loss: 0.2736 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 275/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1780 - accuracy: 0.9539\n",
      "Epoch 00275: val_loss did not improve from 0.27361\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1780 - accuracy: 0.9539 - val_loss: 0.2862 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 276/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1678 - accuracy: 0.9587\n",
      "Epoch 00276: val_loss did not improve from 0.27361\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1686 - accuracy: 0.9582 - val_loss: 0.2869 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 277/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1686 - accuracy: 0.9548\n",
      "Epoch 00277: val_loss did not improve from 0.27361\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1684 - accuracy: 0.9547 - val_loss: 0.2867 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 278/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1773 - accuracy: 0.9519\n",
      "Epoch 00278: val_loss did not improve from 0.27361\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1781 - accuracy: 0.9514 - val_loss: 0.2901 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 279/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1717 - accuracy: 0.9562\n",
      "Epoch 00279: val_loss did not improve from 0.27361\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1720 - accuracy: 0.9561 - val_loss: 0.2867 - val_accuracy: 0.9124 - lr: 2.0000e-04\n",
      "Epoch 280/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1663 - accuracy: 0.9583\n",
      "Epoch 00280: val_loss improved from 0.27361 to 0.27017, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1663 - accuracy: 0.9583 - val_loss: 0.2702 - val_accuracy: 0.9124 - lr: 2.0000e-04\n",
      "Epoch 281/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1797 - accuracy: 0.9532\n",
      "Epoch 00281: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1805 - accuracy: 0.9530 - val_loss: 0.2833 - val_accuracy: 0.9137 - lr: 2.0000e-04\n",
      "Epoch 282/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1728 - accuracy: 0.9526\n",
      "Epoch 00282: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1726 - accuracy: 0.9527 - val_loss: 0.2810 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 283/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9550\n",
      "Epoch 00283: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1711 - accuracy: 0.9550 - val_loss: 0.3044 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 284/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.9500\n",
      "Epoch 00284: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1807 - accuracy: 0.9500 - val_loss: 0.2978 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 285/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1777 - accuracy: 0.9517\n",
      "Epoch 00285: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1779 - accuracy: 0.9514 - val_loss: 0.2856 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 286/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1708 - accuracy: 0.9563\n",
      "Epoch 00286: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1708 - accuracy: 0.9563 - val_loss: 0.3140 - val_accuracy: 0.8918 - lr: 2.0000e-04\n",
      "Epoch 287/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1776 - accuracy: 0.9504\n",
      "Epoch 00287: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1773 - accuracy: 0.9506 - val_loss: 0.2859 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 288/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1715 - accuracy: 0.9576\n",
      "Epoch 00288: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1715 - accuracy: 0.9576 - val_loss: 0.2935 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 289/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1740 - accuracy: 0.9532\n",
      "Epoch 00289: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1742 - accuracy: 0.9530 - val_loss: 0.2865 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 290/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1775 - accuracy: 0.9536\n",
      "Epoch 00290: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1770 - accuracy: 0.9537 - val_loss: 0.3153 - val_accuracy: 0.8866 - lr: 2.0000e-04\n",
      "Epoch 291/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1782 - accuracy: 0.9528\n",
      "Epoch 00291: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1794 - accuracy: 0.9526 - val_loss: 0.2921 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 292/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1691 - accuracy: 0.9554\n",
      "Epoch 00292: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1698 - accuracy: 0.9554 - val_loss: 0.2852 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 293/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1780 - accuracy: 0.9507\n",
      "Epoch 00293: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1777 - accuracy: 0.9507 - val_loss: 0.2780 - val_accuracy: 0.9124 - lr: 2.0000e-04\n",
      "Epoch 294/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1799 - accuracy: 0.9500\n",
      "Epoch 00294: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1799 - accuracy: 0.9500 - val_loss: 0.2748 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 295/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1801 - accuracy: 0.9545\n",
      "Epoch 00295: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1804 - accuracy: 0.9541 - val_loss: 0.2705 - val_accuracy: 0.9149 - lr: 2.0000e-04\n",
      "Epoch 296/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1751 - accuracy: 0.9523\n",
      "Epoch 00296: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1754 - accuracy: 0.9521 - val_loss: 0.2884 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 297/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1659 - accuracy: 0.9596\n",
      "Epoch 00297: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1660 - accuracy: 0.9594 - val_loss: 0.3005 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 298/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1718 - accuracy: 0.9529\n",
      "Epoch 00298: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1714 - accuracy: 0.9529 - val_loss: 0.3032 - val_accuracy: 0.8982 - lr: 2.0000e-04\n",
      "Epoch 299/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1811 - accuracy: 0.9500\n",
      "Epoch 00299: val_loss did not improve from 0.27017\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1811 - accuracy: 0.9500 - val_loss: 0.2990 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 300/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1743 - accuracy: 0.9544\n",
      "Epoch 00300: val_loss improved from 0.27017 to 0.27002, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1743 - accuracy: 0.9544 - val_loss: 0.2700 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 301/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1740 - accuracy: 0.9534\n",
      "Epoch 00301: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1740 - accuracy: 0.9534 - val_loss: 0.3088 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 302/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1761 - accuracy: 0.9528\n",
      "Epoch 00302: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1767 - accuracy: 0.9524 - val_loss: 0.2856 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 303/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1757 - accuracy: 0.9551\n",
      "Epoch 00303: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1752 - accuracy: 0.9553 - val_loss: 0.2787 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 304/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1752 - accuracy: 0.9551\n",
      "Epoch 00304: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1748 - accuracy: 0.9551 - val_loss: 0.2966 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 305/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1709 - accuracy: 0.9551\n",
      "Epoch 00305: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1719 - accuracy: 0.9547 - val_loss: 0.2853 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 306/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1704 - accuracy: 0.9554\n",
      "Epoch 00306: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1704 - accuracy: 0.9554 - val_loss: 0.2809 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 307/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1626 - accuracy: 0.9601\n",
      "Epoch 00307: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1625 - accuracy: 0.9602 - val_loss: 0.3043 - val_accuracy: 0.8982 - lr: 2.0000e-04\n",
      "Epoch 308/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1736 - accuracy: 0.9557\n",
      "Epoch 00308: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1735 - accuracy: 0.9557 - val_loss: 0.3012 - val_accuracy: 0.8853 - lr: 2.0000e-04\n",
      "Epoch 309/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1676 - accuracy: 0.9543\n",
      "Epoch 00309: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1677 - accuracy: 0.9543 - val_loss: 0.2769 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 310/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1720 - accuracy: 0.9564\n",
      "Epoch 00310: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1720 - accuracy: 0.9564 - val_loss: 0.2775 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 311/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1676 - accuracy: 0.9552\n",
      "Epoch 00311: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1698 - accuracy: 0.9543 - val_loss: 0.2827 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 312/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1671 - accuracy: 0.9559\n",
      "Epoch 00312: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1667 - accuracy: 0.9561 - val_loss: 0.2921 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 313/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1726 - accuracy: 0.9555\n",
      "Epoch 00313: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1730 - accuracy: 0.9556 - val_loss: 0.2900 - val_accuracy: 0.8982 - lr: 2.0000e-04\n",
      "Epoch 314/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1801 - accuracy: 0.9512\n",
      "Epoch 00314: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1802 - accuracy: 0.9511 - val_loss: 0.2963 - val_accuracy: 0.8918 - lr: 2.0000e-04\n",
      "Epoch 315/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1769 - accuracy: 0.9527\n",
      "Epoch 00315: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1769 - accuracy: 0.9527 - val_loss: 0.2803 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 316/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1838 - accuracy: 0.9491\n",
      "Epoch 00316: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1848 - accuracy: 0.9490 - val_loss: 0.2783 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 317/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1696 - accuracy: 0.9575\n",
      "Epoch 00317: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1701 - accuracy: 0.9574 - val_loss: 0.2800 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 318/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1644 - accuracy: 0.9591\n",
      "Epoch 00318: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1654 - accuracy: 0.9584 - val_loss: 0.2734 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 319/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1642 - accuracy: 0.9557\n",
      "Epoch 00319: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1643 - accuracy: 0.9557 - val_loss: 0.2756 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 320/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1603 - accuracy: 0.9578\n",
      "Epoch 00320: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1598 - accuracy: 0.9583 - val_loss: 0.2732 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 321/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1684 - accuracy: 0.9578\n",
      "Epoch 00321: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1681 - accuracy: 0.9576 - val_loss: 0.2831 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 322/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1630 - accuracy: 0.9581\n",
      "Epoch 00322: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1623 - accuracy: 0.9584 - val_loss: 0.2710 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 323/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1660 - accuracy: 0.9604\n",
      "Epoch 00323: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1659 - accuracy: 0.9602 - val_loss: 0.2810 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 324/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1734 - accuracy: 0.9523\n",
      "Epoch 00324: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1742 - accuracy: 0.9526 - val_loss: 0.2727 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 325/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1672 - accuracy: 0.9578\n",
      "Epoch 00325: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1667 - accuracy: 0.9580 - val_loss: 0.2734 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 326/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1714 - accuracy: 0.9564\n",
      "Epoch 00326: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1718 - accuracy: 0.9560 - val_loss: 0.2829 - val_accuracy: 0.8982 - lr: 2.0000e-04\n",
      "Epoch 327/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.9592\n",
      "Epoch 00327: val_loss did not improve from 0.27002\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1599 - accuracy: 0.9592 - val_loss: 0.2833 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 328/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1643 - accuracy: 0.9551\n",
      "Epoch 00328: val_loss improved from 0.27002 to 0.26904, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1639 - accuracy: 0.9551 - val_loss: 0.2690 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 329/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1588 - accuracy: 0.9630\n",
      "Epoch 00329: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1583 - accuracy: 0.9632 - val_loss: 0.2781 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 330/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1667 - accuracy: 0.9541\n",
      "Epoch 00330: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1666 - accuracy: 0.9541 - val_loss: 0.2709 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 331/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1615 - accuracy: 0.9613\n",
      "Epoch 00331: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1618 - accuracy: 0.9610 - val_loss: 0.2790 - val_accuracy: 0.9008 - lr: 2.0000e-04\n",
      "Epoch 332/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1645 - accuracy: 0.9570\n",
      "Epoch 00332: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1650 - accuracy: 0.9567 - val_loss: 0.2787 - val_accuracy: 0.8982 - lr: 2.0000e-04\n",
      "Epoch 333/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1647 - accuracy: 0.9552\n",
      "Epoch 00333: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1660 - accuracy: 0.9547 - val_loss: 0.2849 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 334/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1704 - accuracy: 0.9551\n",
      "Epoch 00334: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1701 - accuracy: 0.9553 - val_loss: 0.3141 - val_accuracy: 0.8840 - lr: 2.0000e-04\n",
      "Epoch 335/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1621 - accuracy: 0.9617\n",
      "Epoch 00335: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1624 - accuracy: 0.9617 - val_loss: 0.2823 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 336/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1638 - accuracy: 0.9543\n",
      "Epoch 00336: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1644 - accuracy: 0.9540 - val_loss: 0.2821 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 337/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1640 - accuracy: 0.9596\n",
      "Epoch 00337: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1643 - accuracy: 0.9596 - val_loss: 0.2759 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 338/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1721 - accuracy: 0.9564\n",
      "Epoch 00338: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1721 - accuracy: 0.9564 - val_loss: 0.2730 - val_accuracy: 0.9124 - lr: 2.0000e-04\n",
      "Epoch 339/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1670 - accuracy: 0.9587\n",
      "Epoch 00339: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1670 - accuracy: 0.9587 - val_loss: 0.3139 - val_accuracy: 0.8956 - lr: 2.0000e-04\n",
      "Epoch 340/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1569 - accuracy: 0.9603\n",
      "Epoch 00340: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1573 - accuracy: 0.9599 - val_loss: 0.2926 - val_accuracy: 0.8982 - lr: 2.0000e-04\n",
      "Epoch 341/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1638 - accuracy: 0.9583\n",
      "Epoch 00341: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1633 - accuracy: 0.9586 - val_loss: 0.2871 - val_accuracy: 0.9008 - lr: 2.0000e-04\n",
      "Epoch 342/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1616 - accuracy: 0.9596\n",
      "Epoch 00342: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1618 - accuracy: 0.9596 - val_loss: 0.2720 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 343/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1587 - accuracy: 0.9625\n",
      "Epoch 00343: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1587 - accuracy: 0.9625 - val_loss: 0.2746 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 344/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1574 - accuracy: 0.9632\n",
      "Epoch 00344: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1570 - accuracy: 0.9633 - val_loss: 0.2826 - val_accuracy: 0.9008 - lr: 2.0000e-04\n",
      "Epoch 345/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1699 - accuracy: 0.9571\n",
      "Epoch 00345: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1701 - accuracy: 0.9572 - val_loss: 0.2736 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 346/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1636 - accuracy: 0.9591\n",
      "Epoch 00346: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1641 - accuracy: 0.9586 - val_loss: 0.2947 - val_accuracy: 0.8879 - lr: 2.0000e-04\n",
      "Epoch 347/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1594 - accuracy: 0.9606\n",
      "Epoch 00347: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1600 - accuracy: 0.9603 - val_loss: 0.2705 - val_accuracy: 0.9188 - lr: 2.0000e-04\n",
      "Epoch 348/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1501 - accuracy: 0.9630\n",
      "Epoch 00348: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1494 - accuracy: 0.9633 - val_loss: 0.2846 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 349/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1682 - accuracy: 0.9584\n",
      "Epoch 00349: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1705 - accuracy: 0.9572 - val_loss: 0.2708 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 350/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.9558\n",
      "Epoch 00350: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1654 - accuracy: 0.9554 - val_loss: 0.2780 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 351/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1618 - accuracy: 0.9586\n",
      "Epoch 00351: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1618 - accuracy: 0.9586 - val_loss: 0.2808 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 352/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1551 - accuracy: 0.9630\n",
      "Epoch 00352: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1552 - accuracy: 0.9630 - val_loss: 0.2740 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 353/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1645 - accuracy: 0.9586\n",
      "Epoch 00353: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1641 - accuracy: 0.9589 - val_loss: 0.2822 - val_accuracy: 0.9008 - lr: 2.0000e-04\n",
      "Epoch 354/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1713 - accuracy: 0.9572\n",
      "Epoch 00354: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1713 - accuracy: 0.9572 - val_loss: 0.2837 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 355/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1635 - accuracy: 0.9580\n",
      "Epoch 00355: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1633 - accuracy: 0.9580 - val_loss: 0.2757 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 356/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1559 - accuracy: 0.9607\n",
      "Epoch 00356: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1558 - accuracy: 0.9610 - val_loss: 0.2789 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 357/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1684 - accuracy: 0.9565\n",
      "Epoch 00357: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1678 - accuracy: 0.9567 - val_loss: 0.2865 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 358/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1653 - accuracy: 0.9574\n",
      "Epoch 00358: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1661 - accuracy: 0.9570 - val_loss: 0.2854 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 359/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1635 - accuracy: 0.9584\n",
      "Epoch 00359: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1629 - accuracy: 0.9587 - val_loss: 0.2872 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 360/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1631 - accuracy: 0.9584\n",
      "Epoch 00360: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1632 - accuracy: 0.9582 - val_loss: 0.2813 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 361/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1592 - accuracy: 0.9591\n",
      "Epoch 00361: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1592 - accuracy: 0.9592 - val_loss: 0.2819 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 362/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1523 - accuracy: 0.9628\n",
      "Epoch 00362: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1523 - accuracy: 0.9626 - val_loss: 0.2709 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 363/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1612 - accuracy: 0.9594\n",
      "Epoch 00363: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1610 - accuracy: 0.9593 - val_loss: 0.2759 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 364/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1567 - accuracy: 0.9612\n",
      "Epoch 00364: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1571 - accuracy: 0.9610 - val_loss: 0.2709 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 365/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1721 - accuracy: 0.9590\n",
      "Epoch 00365: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1716 - accuracy: 0.9593 - val_loss: 0.2779 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 366/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1696 - accuracy: 0.9567\n",
      "Epoch 00366: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1692 - accuracy: 0.9569 - val_loss: 0.2820 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 367/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1549 - accuracy: 0.9622\n",
      "Epoch 00367: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1549 - accuracy: 0.9622 - val_loss: 0.2839 - val_accuracy: 0.8982 - lr: 2.0000e-04\n",
      "Epoch 368/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9607\n",
      "Epoch 00368: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1576 - accuracy: 0.9604 - val_loss: 0.2836 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 369/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1542 - accuracy: 0.9619\n",
      "Epoch 00369: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1544 - accuracy: 0.9616 - val_loss: 0.2996 - val_accuracy: 0.8982 - lr: 2.0000e-04\n",
      "Epoch 370/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1564 - accuracy: 0.9604\n",
      "Epoch 00370: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1577 - accuracy: 0.9602 - val_loss: 0.2824 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 371/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1635 - accuracy: 0.9606\n",
      "Epoch 00371: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1643 - accuracy: 0.9603 - val_loss: 0.3209 - val_accuracy: 0.8827 - lr: 2.0000e-04\n",
      "Epoch 372/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1651 - accuracy: 0.9600\n",
      "Epoch 00372: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.1653 - accuracy: 0.9602 - val_loss: 0.2750 - val_accuracy: 0.9008 - lr: 2.0000e-04\n",
      "Epoch 373/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1614 - accuracy: 0.9583\n",
      "Epoch 00373: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1618 - accuracy: 0.9580 - val_loss: 0.2771 - val_accuracy: 0.9008 - lr: 2.0000e-04\n",
      "Epoch 374/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1617 - accuracy: 0.9599\n",
      "Epoch 00374: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1614 - accuracy: 0.9599 - val_loss: 0.2917 - val_accuracy: 0.8956 - lr: 2.0000e-04\n",
      "Epoch 375/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1616 - accuracy: 0.9597\n",
      "Epoch 00375: val_loss did not improve from 0.26904\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1616 - accuracy: 0.9597 - val_loss: 0.2701 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 376/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1591 - accuracy: 0.9597\n",
      "Epoch 00376: val_loss improved from 0.26904 to 0.26300, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1587 - accuracy: 0.9600 - val_loss: 0.2630 - val_accuracy: 0.9124 - lr: 2.0000e-04\n",
      "Epoch 377/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1592 - accuracy: 0.9613\n",
      "Epoch 00377: val_loss did not improve from 0.26300\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1589 - accuracy: 0.9615 - val_loss: 0.2895 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 378/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1576 - accuracy: 0.9601\n",
      "Epoch 00378: val_loss did not improve from 0.26300\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1573 - accuracy: 0.9600 - val_loss: 0.2728 - val_accuracy: 0.9124 - lr: 2.0000e-04\n",
      "Epoch 379/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1556 - accuracy: 0.9617\n",
      "Epoch 00379: val_loss did not improve from 0.26300\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1551 - accuracy: 0.9620 - val_loss: 0.2774 - val_accuracy: 0.9034 - lr: 2.0000e-04\n",
      "Epoch 380/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1623 - accuracy: 0.9571\n",
      "Epoch 00380: val_loss did not improve from 0.26300\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1628 - accuracy: 0.9567 - val_loss: 0.2779 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 381/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1640 - accuracy: 0.9587\n",
      "Epoch 00381: val_loss did not improve from 0.26300\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1642 - accuracy: 0.9584 - val_loss: 0.2793 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 382/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.9561\n",
      "Epoch 00382: val_loss did not improve from 0.26300\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1648 - accuracy: 0.9563 - val_loss: 0.2867 - val_accuracy: 0.8930 - lr: 2.0000e-04\n",
      "Epoch 383/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1607 - accuracy: 0.9607\n",
      "Epoch 00383: val_loss did not improve from 0.26300\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1606 - accuracy: 0.9607 - val_loss: 0.2728 - val_accuracy: 0.9137 - lr: 2.0000e-04\n",
      "Epoch 384/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1650 - accuracy: 0.9603\n",
      "Epoch 00384: val_loss did not improve from 0.26300\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1645 - accuracy: 0.9604 - val_loss: 0.2736 - val_accuracy: 0.9162 - lr: 2.0000e-04\n",
      "Epoch 385/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1592 - accuracy: 0.9583\n",
      "Epoch 00385: val_loss did not improve from 0.26300\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1592 - accuracy: 0.9583 - val_loss: 0.2688 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 386/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1553 - accuracy: 0.9625\n",
      "Epoch 00386: val_loss did not improve from 0.26300\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1551 - accuracy: 0.9623 - val_loss: 0.2663 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 387/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1566 - accuracy: 0.9606\n",
      "Epoch 00387: val_loss did not improve from 0.26300\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1566 - accuracy: 0.9604 - val_loss: 0.2818 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 388/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1621 - accuracy: 0.9583\n",
      "Epoch 00388: val_loss did not improve from 0.26300\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1620 - accuracy: 0.9584 - val_loss: 0.2762 - val_accuracy: 0.8930 - lr: 2.0000e-04\n",
      "Epoch 389/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1588 - accuracy: 0.9597\n",
      "Epoch 00389: val_loss did not improve from 0.26300\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1579 - accuracy: 0.9602 - val_loss: 0.2739 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 390/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1559 - accuracy: 0.9620\n",
      "Epoch 00390: val_loss did not improve from 0.26300\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1558 - accuracy: 0.9619 - val_loss: 0.2851 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 391/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1526 - accuracy: 0.9614\n",
      "Epoch 00391: val_loss improved from 0.26300 to 0.26191, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.1524 - accuracy: 0.9615 - val_loss: 0.2619 - val_accuracy: 0.9124 - lr: 2.0000e-04\n",
      "Epoch 392/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1624 - accuracy: 0.9612\n",
      "Epoch 00392: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1629 - accuracy: 0.9604 - val_loss: 0.2662 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 393/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1555 - accuracy: 0.9606\n",
      "Epoch 00393: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1554 - accuracy: 0.9604 - val_loss: 0.2702 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 394/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1563 - accuracy: 0.9620\n",
      "Epoch 00394: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1558 - accuracy: 0.9622 - val_loss: 0.2813 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 395/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1605 - accuracy: 0.9620\n",
      "Epoch 00395: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1604 - accuracy: 0.9620 - val_loss: 0.2765 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 396/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1607 - accuracy: 0.9578\n",
      "Epoch 00396: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1610 - accuracy: 0.9577 - val_loss: 0.2661 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 397/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1549 - accuracy: 0.9616\n",
      "Epoch 00397: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1549 - accuracy: 0.9616 - val_loss: 0.2650 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 398/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1534 - accuracy: 0.9630\n",
      "Epoch 00398: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1534 - accuracy: 0.9630 - val_loss: 0.2902 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 399/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1643 - accuracy: 0.9575\n",
      "Epoch 00399: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1641 - accuracy: 0.9576 - val_loss: 0.2661 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 400/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1648 - accuracy: 0.9578\n",
      "Epoch 00400: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1646 - accuracy: 0.9580 - val_loss: 0.2815 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 401/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1618 - accuracy: 0.9582\n",
      "Epoch 00401: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1618 - accuracy: 0.9582 - val_loss: 0.2860 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 402/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1567 - accuracy: 0.9616\n",
      "Epoch 00402: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1564 - accuracy: 0.9617 - val_loss: 0.2741 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 403/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1533 - accuracy: 0.9642\n",
      "Epoch 00403: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1533 - accuracy: 0.9642 - val_loss: 0.3044 - val_accuracy: 0.8853 - lr: 2.0000e-04\n",
      "Epoch 404/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1562 - accuracy: 0.9588\n",
      "Epoch 00404: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1562 - accuracy: 0.9589 - val_loss: 0.2850 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 405/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1563 - accuracy: 0.9612\n",
      "Epoch 00405: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1558 - accuracy: 0.9615 - val_loss: 0.2991 - val_accuracy: 0.8930 - lr: 2.0000e-04\n",
      "Epoch 406/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1609 - accuracy: 0.9593\n",
      "Epoch 00406: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1608 - accuracy: 0.9594 - val_loss: 0.3048 - val_accuracy: 0.8892 - lr: 2.0000e-04\n",
      "Epoch 407/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1618 - accuracy: 0.9609\n",
      "Epoch 00407: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1618 - accuracy: 0.9607 - val_loss: 0.2788 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 408/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1595 - accuracy: 0.9601\n",
      "Epoch 00408: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1591 - accuracy: 0.9604 - val_loss: 0.2797 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 409/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 0.9607\n",
      "Epoch 00409: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1540 - accuracy: 0.9607 - val_loss: 0.2872 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 410/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1593 - accuracy: 0.9603\n",
      "Epoch 00410: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1593 - accuracy: 0.9603 - val_loss: 0.2881 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 411/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1620 - accuracy: 0.9577\n",
      "Epoch 00411: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1616 - accuracy: 0.9579 - val_loss: 0.2818 - val_accuracy: 0.9008 - lr: 2.0000e-04\n",
      "Epoch 412/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1601 - accuracy: 0.9588\n",
      "Epoch 00412: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1600 - accuracy: 0.9589 - val_loss: 0.2951 - val_accuracy: 0.8930 - lr: 2.0000e-04\n",
      "Epoch 413/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1523 - accuracy: 0.9620\n",
      "Epoch 00413: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1532 - accuracy: 0.9617 - val_loss: 0.3091 - val_accuracy: 0.8853 - lr: 2.0000e-04\n",
      "Epoch 414/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1608 - accuracy: 0.9597\n",
      "Epoch 00414: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1609 - accuracy: 0.9596 - val_loss: 0.2898 - val_accuracy: 0.8956 - lr: 2.0000e-04\n",
      "Epoch 415/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1532 - accuracy: 0.9620\n",
      "Epoch 00415: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1534 - accuracy: 0.9620 - val_loss: 0.2812 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 416/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1534 - accuracy: 0.9612\n",
      "Epoch 00416: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1536 - accuracy: 0.9610 - val_loss: 0.2937 - val_accuracy: 0.8969 - lr: 2.0000e-04\n",
      "Epoch 417/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1536 - accuracy: 0.9620\n",
      "Epoch 00417: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1536 - accuracy: 0.9620 - val_loss: 0.2935 - val_accuracy: 0.8930 - lr: 2.0000e-04\n",
      "Epoch 418/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1521 - accuracy: 0.9629\n",
      "Epoch 00418: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1523 - accuracy: 0.9629 - val_loss: 0.2793 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 419/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1564 - accuracy: 0.9616\n",
      "Epoch 00419: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1566 - accuracy: 0.9615 - val_loss: 0.2870 - val_accuracy: 0.8956 - lr: 2.0000e-04\n",
      "Epoch 420/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1492 - accuracy: 0.9643\n",
      "Epoch 00420: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1489 - accuracy: 0.9642 - val_loss: 0.2806 - val_accuracy: 0.9046 - lr: 2.0000e-04\n",
      "Epoch 421/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1572 - accuracy: 0.9641\n",
      "Epoch 00421: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1588 - accuracy: 0.9633 - val_loss: 0.2860 - val_accuracy: 0.8982 - lr: 2.0000e-04\n",
      "Epoch 422/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1549 - accuracy: 0.9625\n",
      "Epoch 00422: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1549 - accuracy: 0.9625 - val_loss: 0.2750 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 423/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1563 - accuracy: 0.9610\n",
      "Epoch 00423: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1564 - accuracy: 0.9609 - val_loss: 0.2855 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 424/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1565 - accuracy: 0.9588\n",
      "Epoch 00424: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1562 - accuracy: 0.9590 - val_loss: 0.2755 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 425/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1571 - accuracy: 0.9614\n",
      "Epoch 00425: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1563 - accuracy: 0.9617 - val_loss: 0.2874 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 426/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9587\n",
      "Epoch 00426: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1586 - accuracy: 0.9580 - val_loss: 0.2875 - val_accuracy: 0.9021 - lr: 2.0000e-04\n",
      "Epoch 427/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1629 - accuracy: 0.9580\n",
      "Epoch 00427: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1628 - accuracy: 0.9579 - val_loss: 0.2788 - val_accuracy: 0.9059 - lr: 2.0000e-04\n",
      "Epoch 428/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1546 - accuracy: 0.9630\n",
      "Epoch 00428: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.1546 - accuracy: 0.9632 - val_loss: 0.2668 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 429/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1543 - accuracy: 0.9642\n",
      "Epoch 00429: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1539 - accuracy: 0.9643 - val_loss: 0.2633 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 430/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1563 - accuracy: 0.9626\n",
      "Epoch 00430: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1563 - accuracy: 0.9626 - val_loss: 0.2790 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 431/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1652 - accuracy: 0.9577\n",
      "Epoch 00431: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1652 - accuracy: 0.9577 - val_loss: 0.2766 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 432/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1527 - accuracy: 0.9643\n",
      "Epoch 00432: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1524 - accuracy: 0.9643 - val_loss: 0.3041 - val_accuracy: 0.8930 - lr: 2.0000e-04\n",
      "Epoch 433/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1522 - accuracy: 0.9616\n",
      "Epoch 00433: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1521 - accuracy: 0.9616 - val_loss: 0.2719 - val_accuracy: 0.9124 - lr: 2.0000e-04\n",
      "Epoch 434/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1579 - accuracy: 0.9615\n",
      "Epoch 00434: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1579 - accuracy: 0.9615 - val_loss: 0.2847 - val_accuracy: 0.8995 - lr: 2.0000e-04\n",
      "Epoch 435/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1526 - accuracy: 0.9639\n",
      "Epoch 00435: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1531 - accuracy: 0.9636 - val_loss: 0.2763 - val_accuracy: 0.9137 - lr: 2.0000e-04\n",
      "Epoch 436/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1563 - accuracy: 0.9612\n",
      "Epoch 00436: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1568 - accuracy: 0.9610 - val_loss: 0.2769 - val_accuracy: 0.9085 - lr: 2.0000e-04\n",
      "Epoch 437/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1488 - accuracy: 0.9641\n",
      "Epoch 00437: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1487 - accuracy: 0.9640 - val_loss: 0.2664 - val_accuracy: 0.9188 - lr: 2.0000e-04\n",
      "Epoch 438/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1572 - accuracy: 0.9635\n",
      "Epoch 00438: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1575 - accuracy: 0.9632 - val_loss: 0.2871 - val_accuracy: 0.9008 - lr: 2.0000e-04\n",
      "Epoch 439/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1522 - accuracy: 0.9636\n",
      "Epoch 00439: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1529 - accuracy: 0.9632 - val_loss: 0.2861 - val_accuracy: 0.9098 - lr: 2.0000e-04\n",
      "Epoch 440/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1492 - accuracy: 0.9664\n",
      "Epoch 00440: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1488 - accuracy: 0.9665 - val_loss: 0.2722 - val_accuracy: 0.9162 - lr: 2.0000e-04\n",
      "Epoch 441/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1607 - accuracy: 0.9600\n",
      "Epoch 00441: val_loss did not improve from 0.26191\n",
      "\n",
      "Epoch 00441: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1614 - accuracy: 0.9597 - val_loss: 0.2792 - val_accuracy: 0.9072 - lr: 2.0000e-04\n",
      "Epoch 442/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1468 - accuracy: 0.9681\n",
      "Epoch 00442: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1469 - accuracy: 0.9682 - val_loss: 0.2882 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 443/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1380 - accuracy: 0.9700\n",
      "Epoch 00443: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1382 - accuracy: 0.9696 - val_loss: 0.2731 - val_accuracy: 0.9111 - lr: 1.0000e-04\n",
      "Epoch 444/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1393 - accuracy: 0.9678\n",
      "Epoch 00444: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1395 - accuracy: 0.9678 - val_loss: 0.2789 - val_accuracy: 0.9008 - lr: 1.0000e-04\n",
      "Epoch 445/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1293 - accuracy: 0.9716\n",
      "Epoch 00445: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1299 - accuracy: 0.9715 - val_loss: 0.2692 - val_accuracy: 0.9098 - lr: 1.0000e-04\n",
      "Epoch 446/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1393 - accuracy: 0.9699\n",
      "Epoch 00446: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1393 - accuracy: 0.9699 - val_loss: 0.2658 - val_accuracy: 0.9111 - lr: 1.0000e-04\n",
      "Epoch 447/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1377 - accuracy: 0.9702\n",
      "Epoch 00447: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1377 - accuracy: 0.9702 - val_loss: 0.2649 - val_accuracy: 0.9149 - lr: 1.0000e-04\n",
      "Epoch 448/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 0.9729\n",
      "Epoch 00448: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1281 - accuracy: 0.9729 - val_loss: 0.2713 - val_accuracy: 0.9072 - lr: 1.0000e-04\n",
      "Epoch 449/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1332 - accuracy: 0.9717\n",
      "Epoch 00449: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1335 - accuracy: 0.9715 - val_loss: 0.2630 - val_accuracy: 0.9098 - lr: 1.0000e-04\n",
      "Epoch 450/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1322 - accuracy: 0.9681\n",
      "Epoch 00450: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1320 - accuracy: 0.9680 - val_loss: 0.2743 - val_accuracy: 0.9085 - lr: 1.0000e-04\n",
      "Epoch 451/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9712\n",
      "Epoch 00451: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1310 - accuracy: 0.9709 - val_loss: 0.2627 - val_accuracy: 0.9085 - lr: 1.0000e-04\n",
      "Epoch 452/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1285 - accuracy: 0.9699\n",
      "Epoch 00452: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1285 - accuracy: 0.9699 - val_loss: 0.2645 - val_accuracy: 0.9149 - lr: 1.0000e-04\n",
      "Epoch 453/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9751\n",
      "Epoch 00453: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1204 - accuracy: 0.9752 - val_loss: 0.2653 - val_accuracy: 0.9098 - lr: 1.0000e-04\n",
      "Epoch 454/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9718\n",
      "Epoch 00454: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1289 - accuracy: 0.9718 - val_loss: 0.2664 - val_accuracy: 0.9111 - lr: 1.0000e-04\n",
      "Epoch 455/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9698\n",
      "Epoch 00455: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1337 - accuracy: 0.9698 - val_loss: 0.2773 - val_accuracy: 0.8995 - lr: 1.0000e-04\n",
      "Epoch 456/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1325 - accuracy: 0.9712\n",
      "Epoch 00456: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1321 - accuracy: 0.9712 - val_loss: 0.2712 - val_accuracy: 0.9085 - lr: 1.0000e-04\n",
      "Epoch 457/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9736\n",
      "Epoch 00457: val_loss did not improve from 0.26191\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1239 - accuracy: 0.9736 - val_loss: 0.2630 - val_accuracy: 0.9059 - lr: 1.0000e-04\n",
      "Epoch 458/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1340 - accuracy: 0.9703\n",
      "Epoch 00458: val_loss improved from 0.26191 to 0.25616, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.1347 - accuracy: 0.9702 - val_loss: 0.2562 - val_accuracy: 0.9124 - lr: 1.0000e-04\n",
      "Epoch 459/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1304 - accuracy: 0.9699\n",
      "Epoch 00459: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1319 - accuracy: 0.9692 - val_loss: 0.2632 - val_accuracy: 0.9072 - lr: 1.0000e-04\n",
      "Epoch 460/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1269 - accuracy: 0.9733\n",
      "Epoch 00460: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1269 - accuracy: 0.9733 - val_loss: 0.2671 - val_accuracy: 0.9072 - lr: 1.0000e-04\n",
      "Epoch 461/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9746\n",
      "Epoch 00461: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1265 - accuracy: 0.9739 - val_loss: 0.2596 - val_accuracy: 0.9085 - lr: 1.0000e-04\n",
      "Epoch 462/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1286 - accuracy: 0.9716\n",
      "Epoch 00462: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1282 - accuracy: 0.9718 - val_loss: 0.2828 - val_accuracy: 0.8995 - lr: 1.0000e-04\n",
      "Epoch 463/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1295 - accuracy: 0.9700\n",
      "Epoch 00463: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1300 - accuracy: 0.9699 - val_loss: 0.2716 - val_accuracy: 0.9059 - lr: 1.0000e-04\n",
      "Epoch 464/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9729\n",
      "Epoch 00464: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1259 - accuracy: 0.9732 - val_loss: 0.2717 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 465/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1303 - accuracy: 0.9712\n",
      "Epoch 00465: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1304 - accuracy: 0.9711 - val_loss: 0.2627 - val_accuracy: 0.9162 - lr: 1.0000e-04\n",
      "Epoch 466/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1277 - accuracy: 0.9729\n",
      "Epoch 00466: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1277 - accuracy: 0.9728 - val_loss: 0.2696 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 467/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9712\n",
      "Epoch 00467: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1255 - accuracy: 0.9712 - val_loss: 0.2674 - val_accuracy: 0.9072 - lr: 1.0000e-04\n",
      "Epoch 468/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1280 - accuracy: 0.9711\n",
      "Epoch 00468: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1280 - accuracy: 0.9711 - val_loss: 0.2628 - val_accuracy: 0.9098 - lr: 1.0000e-04\n",
      "Epoch 469/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 0.9704\n",
      "Epoch 00469: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1281 - accuracy: 0.9703 - val_loss: 0.2677 - val_accuracy: 0.9059 - lr: 1.0000e-04\n",
      "Epoch 470/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1281 - accuracy: 0.9713\n",
      "Epoch 00470: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1281 - accuracy: 0.9713 - val_loss: 0.2856 - val_accuracy: 0.8995 - lr: 1.0000e-04\n",
      "Epoch 471/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9745\n",
      "Epoch 00471: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1196 - accuracy: 0.9748 - val_loss: 0.2781 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 472/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1185 - accuracy: 0.9771\n",
      "Epoch 00472: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1188 - accuracy: 0.9766 - val_loss: 0.2606 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 473/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1256 - accuracy: 0.9723\n",
      "Epoch 00473: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1256 - accuracy: 0.9723 - val_loss: 0.2836 - val_accuracy: 0.9085 - lr: 1.0000e-04\n",
      "Epoch 474/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1287 - accuracy: 0.9703\n",
      "Epoch 00474: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1285 - accuracy: 0.9705 - val_loss: 0.2694 - val_accuracy: 0.9085 - lr: 1.0000e-04\n",
      "Epoch 475/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1199 - accuracy: 0.9714\n",
      "Epoch 00475: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1195 - accuracy: 0.9716 - val_loss: 0.2700 - val_accuracy: 0.9059 - lr: 1.0000e-04\n",
      "Epoch 476/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9728\n",
      "Epoch 00476: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1186 - accuracy: 0.9723 - val_loss: 0.2712 - val_accuracy: 0.9046 - lr: 1.0000e-04\n",
      "Epoch 477/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9714\n",
      "Epoch 00477: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1211 - accuracy: 0.9711 - val_loss: 0.2641 - val_accuracy: 0.9059 - lr: 1.0000e-04\n",
      "Epoch 478/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9746\n",
      "Epoch 00478: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1197 - accuracy: 0.9743 - val_loss: 0.2672 - val_accuracy: 0.9137 - lr: 1.0000e-04\n",
      "Epoch 479/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9722\n",
      "Epoch 00479: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1251 - accuracy: 0.9722 - val_loss: 0.2850 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 480/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9735\n",
      "Epoch 00480: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1218 - accuracy: 0.9733 - val_loss: 0.2735 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 481/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1131 - accuracy: 0.9764\n",
      "Epoch 00481: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1134 - accuracy: 0.9762 - val_loss: 0.2813 - val_accuracy: 0.9111 - lr: 1.0000e-04\n",
      "Epoch 482/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9707\n",
      "Epoch 00482: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1261 - accuracy: 0.9711 - val_loss: 0.2728 - val_accuracy: 0.8982 - lr: 1.0000e-04\n",
      "Epoch 483/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9729\n",
      "Epoch 00483: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1207 - accuracy: 0.9731 - val_loss: 0.2614 - val_accuracy: 0.9059 - lr: 1.0000e-04\n",
      "Epoch 484/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1250 - accuracy: 0.9698\n",
      "Epoch 00484: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1250 - accuracy: 0.9698 - val_loss: 0.2697 - val_accuracy: 0.9098 - lr: 1.0000e-04\n",
      "Epoch 485/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9720\n",
      "Epoch 00485: val_loss did not improve from 0.25616\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1261 - accuracy: 0.9718 - val_loss: 0.2660 - val_accuracy: 0.9098 - lr: 1.0000e-04\n",
      "Epoch 486/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1307 - accuracy: 0.9701\n",
      "Epoch 00486: val_loss improved from 0.25616 to 0.25519, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.1305 - accuracy: 0.9702 - val_loss: 0.2552 - val_accuracy: 0.9137 - lr: 1.0000e-04\n",
      "Epoch 487/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9707\n",
      "Epoch 00487: val_loss did not improve from 0.25519\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1258 - accuracy: 0.9711 - val_loss: 0.2667 - val_accuracy: 0.9072 - lr: 1.0000e-04\n",
      "Epoch 488/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 0.9743\n",
      "Epoch 00488: val_loss did not improve from 0.25519\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1195 - accuracy: 0.9743 - val_loss: 0.2693 - val_accuracy: 0.9059 - lr: 1.0000e-04\n",
      "Epoch 489/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9736\n",
      "Epoch 00489: val_loss did not improve from 0.25519\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1251 - accuracy: 0.9736 - val_loss: 0.2723 - val_accuracy: 0.9072 - lr: 1.0000e-04\n",
      "Epoch 490/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1161 - accuracy: 0.9748\n",
      "Epoch 00490: val_loss did not improve from 0.25519\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1163 - accuracy: 0.9748 - val_loss: 0.2605 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 491/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1146 - accuracy: 0.9754\n",
      "Epoch 00491: val_loss did not improve from 0.25519\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1144 - accuracy: 0.9755 - val_loss: 0.2657 - val_accuracy: 0.9085 - lr: 1.0000e-04\n",
      "Epoch 492/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1141 - accuracy: 0.9751\n",
      "Epoch 00492: val_loss did not improve from 0.25519\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1143 - accuracy: 0.9748 - val_loss: 0.2627 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 493/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9716\n",
      "Epoch 00493: val_loss did not improve from 0.25519\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1236 - accuracy: 0.9716 - val_loss: 0.2779 - val_accuracy: 0.8982 - lr: 1.0000e-04\n",
      "Epoch 494/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9725\n",
      "Epoch 00494: val_loss did not improve from 0.25519\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1197 - accuracy: 0.9725 - val_loss: 0.2669 - val_accuracy: 0.9046 - lr: 1.0000e-04\n",
      "Epoch 495/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9742\n",
      "Epoch 00495: val_loss did not improve from 0.25519\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1220 - accuracy: 0.9742 - val_loss: 0.2701 - val_accuracy: 0.9046 - lr: 1.0000e-04\n",
      "Epoch 496/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9741\n",
      "Epoch 00496: val_loss did not improve from 0.25519\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1209 - accuracy: 0.9741 - val_loss: 0.2598 - val_accuracy: 0.9085 - lr: 1.0000e-04\n",
      "Epoch 497/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1222 - accuracy: 0.9718\n",
      "Epoch 00497: val_loss improved from 0.25519 to 0.25356, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.1222 - accuracy: 0.9718 - val_loss: 0.2536 - val_accuracy: 0.9124 - lr: 1.0000e-04\n",
      "Epoch 498/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1199 - accuracy: 0.9717\n",
      "Epoch 00498: val_loss did not improve from 0.25356\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1201 - accuracy: 0.9715 - val_loss: 0.2869 - val_accuracy: 0.8995 - lr: 1.0000e-04\n",
      "Epoch 499/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9759\n",
      "Epoch 00499: val_loss did not improve from 0.25356\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1177 - accuracy: 0.9761 - val_loss: 0.2692 - val_accuracy: 0.9008 - lr: 1.0000e-04\n",
      "Epoch 500/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1167 - accuracy: 0.9738\n",
      "Epoch 00500: val_loss did not improve from 0.25356\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1167 - accuracy: 0.9738 - val_loss: 0.2657 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 501/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1256 - accuracy: 0.9703\n",
      "Epoch 00501: val_loss did not improve from 0.25356\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1256 - accuracy: 0.9703 - val_loss: 0.2563 - val_accuracy: 0.9098 - lr: 1.0000e-04\n",
      "Epoch 502/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9723\n",
      "Epoch 00502: val_loss did not improve from 0.25356\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1195 - accuracy: 0.9723 - val_loss: 0.2716 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 503/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1164 - accuracy: 0.9731\n",
      "Epoch 00503: val_loss did not improve from 0.25356\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1164 - accuracy: 0.9731 - val_loss: 0.2674 - val_accuracy: 0.9072 - lr: 1.0000e-04\n",
      "Epoch 504/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1162 - accuracy: 0.9761\n",
      "Epoch 00504: val_loss did not improve from 0.25356\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1160 - accuracy: 0.9762 - val_loss: 0.2570 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 505/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1174 - accuracy: 0.9735\n",
      "Epoch 00505: val_loss improved from 0.25356 to 0.24604, saving model to ././edit18_PC_6_model_n_best_weights.h5\n",
      "70/70 [==============================] - 3s 44ms/step - loss: 0.1174 - accuracy: 0.9735 - val_loss: 0.2460 - val_accuracy: 0.9098 - lr: 1.0000e-04\n",
      "Epoch 506/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9719\n",
      "Epoch 00506: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.1229 - accuracy: 0.9715 - val_loss: 0.2559 - val_accuracy: 0.9085 - lr: 1.0000e-04\n",
      "Epoch 507/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9754\n",
      "Epoch 00507: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1142 - accuracy: 0.9754 - val_loss: 0.2617 - val_accuracy: 0.9046 - lr: 1.0000e-04\n",
      "Epoch 508/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1231 - accuracy: 0.9728\n",
      "Epoch 00508: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1231 - accuracy: 0.9728 - val_loss: 0.2673 - val_accuracy: 0.9008 - lr: 1.0000e-04\n",
      "Epoch 509/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9722\n",
      "Epoch 00509: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1253 - accuracy: 0.9722 - val_loss: 0.2703 - val_accuracy: 0.9021 - lr: 1.0000e-04\n",
      "Epoch 510/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9742\n",
      "Epoch 00510: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1172 - accuracy: 0.9743 - val_loss: 0.2732 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 511/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1121 - accuracy: 0.9751\n",
      "Epoch 00511: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1120 - accuracy: 0.9751 - val_loss: 0.2609 - val_accuracy: 0.9098 - lr: 1.0000e-04\n",
      "Epoch 512/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9729\n",
      "Epoch 00512: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1225 - accuracy: 0.9729 - val_loss: 0.2653 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 513/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1181 - accuracy: 0.9745\n",
      "Epoch 00513: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1179 - accuracy: 0.9745 - val_loss: 0.2507 - val_accuracy: 0.9111 - lr: 1.0000e-04\n",
      "Epoch 514/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1209 - accuracy: 0.9715\n",
      "Epoch 00514: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1209 - accuracy: 0.9715 - val_loss: 0.2614 - val_accuracy: 0.9072 - lr: 1.0000e-04\n",
      "Epoch 515/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1163 - accuracy: 0.9743\n",
      "Epoch 00515: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1163 - accuracy: 0.9743 - val_loss: 0.2585 - val_accuracy: 0.9046 - lr: 1.0000e-04\n",
      "Epoch 516/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9735\n",
      "Epoch 00516: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1206 - accuracy: 0.9736 - val_loss: 0.2586 - val_accuracy: 0.9124 - lr: 1.0000e-04\n",
      "Epoch 517/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1218 - accuracy: 0.9719\n",
      "Epoch 00517: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1218 - accuracy: 0.9719 - val_loss: 0.2802 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 518/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1170 - accuracy: 0.9743\n",
      "Epoch 00518: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1166 - accuracy: 0.9745 - val_loss: 0.2536 - val_accuracy: 0.9072 - lr: 1.0000e-04\n",
      "Epoch 519/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9733\n",
      "Epoch 00519: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1195 - accuracy: 0.9735 - val_loss: 0.2701 - val_accuracy: 0.8995 - lr: 1.0000e-04\n",
      "Epoch 520/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1160 - accuracy: 0.9732\n",
      "Epoch 00520: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1156 - accuracy: 0.9733 - val_loss: 0.2573 - val_accuracy: 0.9072 - lr: 1.0000e-04\n",
      "Epoch 521/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1225 - accuracy: 0.9706\n",
      "Epoch 00521: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1225 - accuracy: 0.9706 - val_loss: 0.2699 - val_accuracy: 0.9008 - lr: 1.0000e-04\n",
      "Epoch 522/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1110 - accuracy: 0.9758\n",
      "Epoch 00522: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1120 - accuracy: 0.9755 - val_loss: 0.2603 - val_accuracy: 0.9111 - lr: 1.0000e-04\n",
      "Epoch 523/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1131 - accuracy: 0.9767\n",
      "Epoch 00523: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1142 - accuracy: 0.9762 - val_loss: 0.2795 - val_accuracy: 0.9021 - lr: 1.0000e-04\n",
      "Epoch 524/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1136 - accuracy: 0.9749\n",
      "Epoch 00524: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1135 - accuracy: 0.9748 - val_loss: 0.2602 - val_accuracy: 0.9124 - lr: 1.0000e-04\n",
      "Epoch 525/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1131 - accuracy: 0.9754\n",
      "Epoch 00525: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1129 - accuracy: 0.9755 - val_loss: 0.2628 - val_accuracy: 0.9098 - lr: 1.0000e-04\n",
      "Epoch 526/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9719\n",
      "Epoch 00526: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1205 - accuracy: 0.9716 - val_loss: 0.2701 - val_accuracy: 0.9085 - lr: 1.0000e-04\n",
      "Epoch 527/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1091 - accuracy: 0.9764\n",
      "Epoch 00527: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1088 - accuracy: 0.9765 - val_loss: 0.2620 - val_accuracy: 0.9059 - lr: 1.0000e-04\n",
      "Epoch 528/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9726\n",
      "Epoch 00528: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1191 - accuracy: 0.9723 - val_loss: 0.2706 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 529/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9746\n",
      "Epoch 00529: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1181 - accuracy: 0.9749 - val_loss: 0.2584 - val_accuracy: 0.9072 - lr: 1.0000e-04\n",
      "Epoch 530/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1160 - accuracy: 0.9722\n",
      "Epoch 00530: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1161 - accuracy: 0.9719 - val_loss: 0.2774 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 531/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1131 - accuracy: 0.9754\n",
      "Epoch 00531: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1131 - accuracy: 0.9754 - val_loss: 0.2661 - val_accuracy: 0.8995 - lr: 1.0000e-04\n",
      "Epoch 532/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1199 - accuracy: 0.9714\n",
      "Epoch 00532: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1207 - accuracy: 0.9711 - val_loss: 0.2770 - val_accuracy: 0.9008 - lr: 1.0000e-04\n",
      "Epoch 533/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1160 - accuracy: 0.9722\n",
      "Epoch 00533: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1165 - accuracy: 0.9721 - val_loss: 0.2848 - val_accuracy: 0.8969 - lr: 1.0000e-04\n",
      "Epoch 534/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1163 - accuracy: 0.9745\n",
      "Epoch 00534: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1161 - accuracy: 0.9746 - val_loss: 0.2642 - val_accuracy: 0.9072 - lr: 1.0000e-04\n",
      "Epoch 535/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1079 - accuracy: 0.9764\n",
      "Epoch 00535: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1079 - accuracy: 0.9764 - val_loss: 0.2784 - val_accuracy: 0.8995 - lr: 1.0000e-04\n",
      "Epoch 536/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1115 - accuracy: 0.9748\n",
      "Epoch 00536: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1115 - accuracy: 0.9748 - val_loss: 0.2634 - val_accuracy: 0.9085 - lr: 1.0000e-04\n",
      "Epoch 537/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1128 - accuracy: 0.9729\n",
      "Epoch 00537: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1128 - accuracy: 0.9729 - val_loss: 0.2744 - val_accuracy: 0.9059 - lr: 1.0000e-04\n",
      "Epoch 538/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1132 - accuracy: 0.9745\n",
      "Epoch 00538: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1132 - accuracy: 0.9745 - val_loss: 0.2599 - val_accuracy: 0.9111 - lr: 1.0000e-04\n",
      "Epoch 539/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1204 - accuracy: 0.9730\n",
      "Epoch 00539: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1205 - accuracy: 0.9731 - val_loss: 0.2698 - val_accuracy: 0.9072 - lr: 1.0000e-04\n",
      "Epoch 540/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1120 - accuracy: 0.9755\n",
      "Epoch 00540: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1121 - accuracy: 0.9755 - val_loss: 0.2668 - val_accuracy: 0.9098 - lr: 1.0000e-04\n",
      "Epoch 541/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9779\n",
      "Epoch 00541: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1043 - accuracy: 0.9779 - val_loss: 0.2818 - val_accuracy: 0.8982 - lr: 1.0000e-04\n",
      "Epoch 542/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9743\n",
      "Epoch 00542: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1170 - accuracy: 0.9745 - val_loss: 0.2823 - val_accuracy: 0.8969 - lr: 1.0000e-04\n",
      "Epoch 543/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1147 - accuracy: 0.9745\n",
      "Epoch 00543: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1143 - accuracy: 0.9748 - val_loss: 0.2551 - val_accuracy: 0.9034 - lr: 1.0000e-04\n",
      "Epoch 544/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9709\n",
      "Epoch 00544: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1230 - accuracy: 0.9708 - val_loss: 0.2592 - val_accuracy: 0.9085 - lr: 1.0000e-04\n",
      "Epoch 545/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9713\n",
      "Epoch 00545: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1179 - accuracy: 0.9713 - val_loss: 0.2618 - val_accuracy: 0.9098 - lr: 1.0000e-04\n",
      "Epoch 546/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1134 - accuracy: 0.9746\n",
      "Epoch 00546: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1134 - accuracy: 0.9748 - val_loss: 0.2614 - val_accuracy: 0.9149 - lr: 1.0000e-04\n",
      "Epoch 547/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1064 - accuracy: 0.9778\n",
      "Epoch 00547: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1067 - accuracy: 0.9776 - val_loss: 0.2676 - val_accuracy: 0.9059 - lr: 1.0000e-04\n",
      "Epoch 548/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1123 - accuracy: 0.9758\n",
      "Epoch 00548: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1119 - accuracy: 0.9761 - val_loss: 0.2827 - val_accuracy: 0.9046 - lr: 1.0000e-04\n",
      "Epoch 549/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1070 - accuracy: 0.9771\n",
      "Epoch 00549: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1068 - accuracy: 0.9772 - val_loss: 0.2664 - val_accuracy: 0.9021 - lr: 1.0000e-04\n",
      "Epoch 550/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1125 - accuracy: 0.9764\n",
      "Epoch 00550: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1124 - accuracy: 0.9764 - val_loss: 0.2649 - val_accuracy: 0.9059 - lr: 1.0000e-04\n",
      "Epoch 551/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1131 - accuracy: 0.9736\n",
      "Epoch 00551: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1136 - accuracy: 0.9732 - val_loss: 0.2624 - val_accuracy: 0.9098 - lr: 1.0000e-04\n",
      "Epoch 552/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1120 - accuracy: 0.9746\n",
      "Epoch 00552: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1120 - accuracy: 0.9746 - val_loss: 0.2651 - val_accuracy: 0.9072 - lr: 1.0000e-04\n",
      "Epoch 553/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1160 - accuracy: 0.9730\n",
      "Epoch 00553: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1158 - accuracy: 0.9732 - val_loss: 0.2637 - val_accuracy: 0.9085 - lr: 1.0000e-04\n",
      "Epoch 554/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1158 - accuracy: 0.9738\n",
      "Epoch 00554: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1165 - accuracy: 0.9732 - val_loss: 0.2588 - val_accuracy: 0.9085 - lr: 1.0000e-04\n",
      "Epoch 555/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1107 - accuracy: 0.9723\n",
      "Epoch 00555: val_loss did not improve from 0.24604\n",
      "\n",
      "Epoch 00555: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1112 - accuracy: 0.9719 - val_loss: 0.2703 - val_accuracy: 0.9059 - lr: 1.0000e-04\n",
      "Epoch 556/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1101 - accuracy: 0.9749\n",
      "Epoch 00556: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1102 - accuracy: 0.9749 - val_loss: 0.2663 - val_accuracy: 0.9137 - lr: 5.0000e-05\n",
      "Epoch 557/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1102 - accuracy: 0.9743\n",
      "Epoch 00557: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1096 - accuracy: 0.9746 - val_loss: 0.2780 - val_accuracy: 0.8969 - lr: 5.0000e-05\n",
      "Epoch 558/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1071 - accuracy: 0.9772\n",
      "Epoch 00558: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1081 - accuracy: 0.9768 - val_loss: 0.2606 - val_accuracy: 0.8969 - lr: 5.0000e-05\n",
      "Epoch 559/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1046 - accuracy: 0.9774\n",
      "Epoch 00559: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1046 - accuracy: 0.9774 - val_loss: 0.2649 - val_accuracy: 0.9021 - lr: 5.0000e-05\n",
      "Epoch 560/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0997 - accuracy: 0.9812\n",
      "Epoch 00560: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0998 - accuracy: 0.9812 - val_loss: 0.2596 - val_accuracy: 0.9034 - lr: 5.0000e-05\n",
      "Epoch 561/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1077 - accuracy: 0.9775\n",
      "Epoch 00561: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1078 - accuracy: 0.9774 - val_loss: 0.2617 - val_accuracy: 0.9046 - lr: 5.0000e-05\n",
      "Epoch 562/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1073 - accuracy: 0.9771\n",
      "Epoch 00562: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1078 - accuracy: 0.9768 - val_loss: 0.2586 - val_accuracy: 0.9008 - lr: 5.0000e-05\n",
      "Epoch 563/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0995 - accuracy: 0.9803\n",
      "Epoch 00563: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0996 - accuracy: 0.9802 - val_loss: 0.2571 - val_accuracy: 0.9059 - lr: 5.0000e-05\n",
      "Epoch 564/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1037 - accuracy: 0.9781\n",
      "Epoch 00564: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1038 - accuracy: 0.9779 - val_loss: 0.2617 - val_accuracy: 0.9072 - lr: 5.0000e-05\n",
      "Epoch 565/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1083 - accuracy: 0.9764\n",
      "Epoch 00565: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1080 - accuracy: 0.9766 - val_loss: 0.2608 - val_accuracy: 0.9046 - lr: 5.0000e-05\n",
      "Epoch 566/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0999 - accuracy: 0.9780\n",
      "Epoch 00566: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1002 - accuracy: 0.9779 - val_loss: 0.2612 - val_accuracy: 0.9046 - lr: 5.0000e-05\n",
      "Epoch 567/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1057 - accuracy: 0.9761\n",
      "Epoch 00567: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1052 - accuracy: 0.9764 - val_loss: 0.2598 - val_accuracy: 0.9059 - lr: 5.0000e-05\n",
      "Epoch 568/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9811\n",
      "Epoch 00568: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 43ms/step - loss: 0.1004 - accuracy: 0.9811 - val_loss: 0.2664 - val_accuracy: 0.9059 - lr: 5.0000e-05\n",
      "Epoch 569/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1033 - accuracy: 0.9784\n",
      "Epoch 00569: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1033 - accuracy: 0.9784 - val_loss: 0.2701 - val_accuracy: 0.9008 - lr: 5.0000e-05\n",
      "Epoch 570/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0959 - accuracy: 0.9809\n",
      "Epoch 00570: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0958 - accuracy: 0.9809 - val_loss: 0.2543 - val_accuracy: 0.9085 - lr: 5.0000e-05\n",
      "Epoch 571/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.0979 - accuracy: 0.9809\n",
      "Epoch 00571: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0979 - accuracy: 0.9809 - val_loss: 0.2722 - val_accuracy: 0.8995 - lr: 5.0000e-05\n",
      "Epoch 572/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1005 - accuracy: 0.9791\n",
      "Epoch 00572: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1002 - accuracy: 0.9792 - val_loss: 0.2608 - val_accuracy: 0.9124 - lr: 5.0000e-05\n",
      "Epoch 573/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0991 - accuracy: 0.9817\n",
      "Epoch 00573: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0990 - accuracy: 0.9817 - val_loss: 0.2606 - val_accuracy: 0.9034 - lr: 5.0000e-05\n",
      "Epoch 574/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1018 - accuracy: 0.9775\n",
      "Epoch 00574: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1017 - accuracy: 0.9776 - val_loss: 0.2636 - val_accuracy: 0.9021 - lr: 5.0000e-05\n",
      "Epoch 575/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0974 - accuracy: 0.9783\n",
      "Epoch 00575: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0971 - accuracy: 0.9785 - val_loss: 0.2580 - val_accuracy: 0.9072 - lr: 5.0000e-05\n",
      "Epoch 576/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0970 - accuracy: 0.9801\n",
      "Epoch 00576: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0971 - accuracy: 0.9798 - val_loss: 0.2596 - val_accuracy: 0.9072 - lr: 5.0000e-05\n",
      "Epoch 577/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1035 - accuracy: 0.9780\n",
      "Epoch 00577: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1038 - accuracy: 0.9778 - val_loss: 0.2548 - val_accuracy: 0.9021 - lr: 5.0000e-05\n",
      "Epoch 578/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0935 - accuracy: 0.9825\n",
      "Epoch 00578: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0934 - accuracy: 0.9824 - val_loss: 0.2665 - val_accuracy: 0.9098 - lr: 5.0000e-05\n",
      "Epoch 579/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0985 - accuracy: 0.9793\n",
      "Epoch 00579: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0983 - accuracy: 0.9792 - val_loss: 0.2636 - val_accuracy: 0.9072 - lr: 5.0000e-05\n",
      "Epoch 580/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1014 - accuracy: 0.9799\n",
      "Epoch 00580: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1011 - accuracy: 0.9801 - val_loss: 0.2664 - val_accuracy: 0.9072 - lr: 5.0000e-05\n",
      "Epoch 581/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1022 - accuracy: 0.9799\n",
      "Epoch 00581: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.1035 - accuracy: 0.9797 - val_loss: 0.2705 - val_accuracy: 0.9059 - lr: 5.0000e-05\n",
      "Epoch 582/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1005 - accuracy: 0.9793\n",
      "Epoch 00582: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1001 - accuracy: 0.9795 - val_loss: 0.2712 - val_accuracy: 0.9008 - lr: 5.0000e-05\n",
      "Epoch 583/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1003 - accuracy: 0.9817\n",
      "Epoch 00583: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1002 - accuracy: 0.9817 - val_loss: 0.2629 - val_accuracy: 0.9098 - lr: 5.0000e-05\n",
      "Epoch 584/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1000 - accuracy: 0.9786\n",
      "Epoch 00584: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1003 - accuracy: 0.9785 - val_loss: 0.2597 - val_accuracy: 0.9059 - lr: 5.0000e-05\n",
      "Epoch 585/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0982 - accuracy: 0.9801\n",
      "Epoch 00585: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0983 - accuracy: 0.9799 - val_loss: 0.2641 - val_accuracy: 0.9059 - lr: 5.0000e-05\n",
      "Epoch 586/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9794\n",
      "Epoch 00586: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0993 - accuracy: 0.9794 - val_loss: 0.2464 - val_accuracy: 0.9059 - lr: 5.0000e-05\n",
      "Epoch 587/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1010 - accuracy: 0.9771\n",
      "Epoch 00587: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1008 - accuracy: 0.9774 - val_loss: 0.2489 - val_accuracy: 0.9124 - lr: 5.0000e-05\n",
      "Epoch 588/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0999 - accuracy: 0.9800\n",
      "Epoch 00588: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0999 - accuracy: 0.9799 - val_loss: 0.2578 - val_accuracy: 0.9124 - lr: 5.0000e-05\n",
      "Epoch 589/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1004 - accuracy: 0.9791\n",
      "Epoch 00589: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.1001 - accuracy: 0.9794 - val_loss: 0.2603 - val_accuracy: 0.9098 - lr: 5.0000e-05\n",
      "Epoch 590/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.9766\n",
      "Epoch 00590: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.1017 - accuracy: 0.9766 - val_loss: 0.2590 - val_accuracy: 0.9085 - lr: 5.0000e-05\n",
      "Epoch 591/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0930 - accuracy: 0.9809\n",
      "Epoch 00591: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0928 - accuracy: 0.9811 - val_loss: 0.2612 - val_accuracy: 0.9046 - lr: 5.0000e-05\n",
      "Epoch 592/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0980 - accuracy: 0.9783\n",
      "Epoch 00592: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0975 - accuracy: 0.9785 - val_loss: 0.2575 - val_accuracy: 0.9059 - lr: 5.0000e-05\n",
      "Epoch 593/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0949 - accuracy: 0.9820\n",
      "Epoch 00593: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0946 - accuracy: 0.9821 - val_loss: 0.2519 - val_accuracy: 0.9046 - lr: 5.0000e-05\n",
      "Epoch 594/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0954 - accuracy: 0.9806\n",
      "Epoch 00594: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0952 - accuracy: 0.9805 - val_loss: 0.2538 - val_accuracy: 0.9085 - lr: 5.0000e-05\n",
      "Epoch 595/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0963 - accuracy: 0.9813\n",
      "Epoch 00595: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0965 - accuracy: 0.9812 - val_loss: 0.2669 - val_accuracy: 0.9046 - lr: 5.0000e-05\n",
      "Epoch 596/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.0924 - accuracy: 0.9815\n",
      "Epoch 00596: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0924 - accuracy: 0.9815 - val_loss: 0.2629 - val_accuracy: 0.9046 - lr: 5.0000e-05\n",
      "Epoch 597/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0922 - accuracy: 0.9813\n",
      "Epoch 00597: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0918 - accuracy: 0.9815 - val_loss: 0.2637 - val_accuracy: 0.9034 - lr: 5.0000e-05\n",
      "Epoch 598/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0945 - accuracy: 0.9819\n",
      "Epoch 00598: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0941 - accuracy: 0.9819 - val_loss: 0.2528 - val_accuracy: 0.9046 - lr: 5.0000e-05\n",
      "Epoch 599/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.1010 - accuracy: 0.9800\n",
      "Epoch 00599: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.1012 - accuracy: 0.9798 - val_loss: 0.2652 - val_accuracy: 0.9034 - lr: 5.0000e-05\n",
      "Epoch 600/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0958 - accuracy: 0.9794\n",
      "Epoch 00600: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 42ms/step - loss: 0.0960 - accuracy: 0.9791 - val_loss: 0.2570 - val_accuracy: 0.9085 - lr: 5.0000e-05\n",
      "Epoch 601/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0927 - accuracy: 0.9822\n",
      "Epoch 00601: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0932 - accuracy: 0.9821 - val_loss: 0.2621 - val_accuracy: 0.9046 - lr: 5.0000e-05\n",
      "Epoch 602/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0974 - accuracy: 0.9791\n",
      "Epoch 00602: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0971 - accuracy: 0.9794 - val_loss: 0.2516 - val_accuracy: 0.9098 - lr: 5.0000e-05\n",
      "Epoch 603/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0906 - accuracy: 0.9823\n",
      "Epoch 00603: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0909 - accuracy: 0.9822 - val_loss: 0.2606 - val_accuracy: 0.9059 - lr: 5.0000e-05\n",
      "Epoch 604/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0923 - accuracy: 0.9817\n",
      "Epoch 00604: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0919 - accuracy: 0.9819 - val_loss: 0.2574 - val_accuracy: 0.9072 - lr: 5.0000e-05\n",
      "Epoch 605/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0966 - accuracy: 0.9807\n",
      "Epoch 00605: val_loss did not improve from 0.24604\n",
      "\n",
      "Epoch 00605: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0963 - accuracy: 0.9808 - val_loss: 0.2651 - val_accuracy: 0.9021 - lr: 5.0000e-05\n",
      "Epoch 606/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0916 - accuracy: 0.9813\n",
      "Epoch 00606: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0912 - accuracy: 0.9815 - val_loss: 0.2621 - val_accuracy: 0.9046 - lr: 2.5000e-05\n",
      "Epoch 607/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0932 - accuracy: 0.9838\n",
      "Epoch 00607: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0929 - accuracy: 0.9838 - val_loss: 0.2609 - val_accuracy: 0.9098 - lr: 2.5000e-05\n",
      "Epoch 608/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0930 - accuracy: 0.9806\n",
      "Epoch 00608: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0932 - accuracy: 0.9804 - val_loss: 0.2581 - val_accuracy: 0.9072 - lr: 2.5000e-05\n",
      "Epoch 609/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0885 - accuracy: 0.9839\n",
      "Epoch 00609: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.0887 - accuracy: 0.9839 - val_loss: 0.2606 - val_accuracy: 0.8995 - lr: 2.5000e-05\n",
      "Epoch 610/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0892 - accuracy: 0.9823\n",
      "Epoch 00610: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0891 - accuracy: 0.9824 - val_loss: 0.2624 - val_accuracy: 0.9021 - lr: 2.5000e-05\n",
      "Epoch 611/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0929 - accuracy: 0.9823\n",
      "Epoch 00611: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0932 - accuracy: 0.9821 - val_loss: 0.2590 - val_accuracy: 0.9072 - lr: 2.5000e-05\n",
      "Epoch 612/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0920 - accuracy: 0.9822\n",
      "Epoch 00612: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0918 - accuracy: 0.9821 - val_loss: 0.2599 - val_accuracy: 0.9046 - lr: 2.5000e-05\n",
      "Epoch 613/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0900 - accuracy: 0.9828\n",
      "Epoch 00613: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0900 - accuracy: 0.9828 - val_loss: 0.2598 - val_accuracy: 0.9059 - lr: 2.5000e-05\n",
      "Epoch 614/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0928 - accuracy: 0.9823\n",
      "Epoch 00614: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0924 - accuracy: 0.9825 - val_loss: 0.2540 - val_accuracy: 0.9059 - lr: 2.5000e-05\n",
      "Epoch 615/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0909 - accuracy: 0.9825\n",
      "Epoch 00615: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0906 - accuracy: 0.9825 - val_loss: 0.2580 - val_accuracy: 0.9059 - lr: 2.5000e-05\n",
      "Epoch 616/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0913 - accuracy: 0.9807\n",
      "Epoch 00616: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0914 - accuracy: 0.9807 - val_loss: 0.2597 - val_accuracy: 0.9072 - lr: 2.5000e-05\n",
      "Epoch 617/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0948 - accuracy: 0.9800\n",
      "Epoch 00617: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0945 - accuracy: 0.9802 - val_loss: 0.2533 - val_accuracy: 0.9098 - lr: 2.5000e-05\n",
      "Epoch 618/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.9830\n",
      "Epoch 00618: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0882 - accuracy: 0.9831 - val_loss: 0.2613 - val_accuracy: 0.9124 - lr: 2.5000e-05\n",
      "Epoch 619/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.0913 - accuracy: 0.9817\n",
      "Epoch 00619: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0913 - accuracy: 0.9817 - val_loss: 0.2656 - val_accuracy: 0.9059 - lr: 2.5000e-05\n",
      "Epoch 620/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0925 - accuracy: 0.9813\n",
      "Epoch 00620: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0922 - accuracy: 0.9815 - val_loss: 0.2618 - val_accuracy: 0.9046 - lr: 2.5000e-05\n",
      "Epoch 621/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0894 - accuracy: 0.9825\n",
      "Epoch 00621: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0899 - accuracy: 0.9821 - val_loss: 0.2579 - val_accuracy: 0.9046 - lr: 2.5000e-05\n",
      "Epoch 622/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0910 - accuracy: 0.9817\n",
      "Epoch 00622: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0909 - accuracy: 0.9817 - val_loss: 0.2574 - val_accuracy: 0.9072 - lr: 2.5000e-05\n",
      "Epoch 623/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.0912 - accuracy: 0.9811\n",
      "Epoch 00623: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0912 - accuracy: 0.9811 - val_loss: 0.2539 - val_accuracy: 0.9111 - lr: 2.5000e-05\n",
      "Epoch 624/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9804\n",
      "Epoch 00624: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0921 - accuracy: 0.9804 - val_loss: 0.2547 - val_accuracy: 0.9046 - lr: 2.5000e-05\n",
      "Epoch 625/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9838\n",
      "Epoch 00625: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0870 - accuracy: 0.9837 - val_loss: 0.2529 - val_accuracy: 0.9085 - lr: 2.5000e-05\n",
      "Epoch 626/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0918 - accuracy: 0.9820\n",
      "Epoch 00626: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0920 - accuracy: 0.9817 - val_loss: 0.2607 - val_accuracy: 0.9046 - lr: 2.5000e-05\n",
      "Epoch 627/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0913 - accuracy: 0.9833\n",
      "Epoch 00627: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0910 - accuracy: 0.9835 - val_loss: 0.2603 - val_accuracy: 0.9072 - lr: 2.5000e-05\n",
      "Epoch 628/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0882 - accuracy: 0.9817\n",
      "Epoch 00628: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0879 - accuracy: 0.9818 - val_loss: 0.2517 - val_accuracy: 0.9085 - lr: 2.5000e-05\n",
      "Epoch 629/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0903 - accuracy: 0.9804\n",
      "Epoch 00629: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0901 - accuracy: 0.9805 - val_loss: 0.2521 - val_accuracy: 0.9072 - lr: 2.5000e-05\n",
      "Epoch 630/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0896 - accuracy: 0.9836\n",
      "Epoch 00630: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0907 - accuracy: 0.9834 - val_loss: 0.2577 - val_accuracy: 0.9085 - lr: 2.5000e-05\n",
      "Epoch 631/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0909 - accuracy: 0.9799\n",
      "Epoch 00631: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0912 - accuracy: 0.9798 - val_loss: 0.2639 - val_accuracy: 0.9072 - lr: 2.5000e-05\n",
      "Epoch 632/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0880 - accuracy: 0.9830\n",
      "Epoch 00632: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0883 - accuracy: 0.9828 - val_loss: 0.2627 - val_accuracy: 0.9046 - lr: 2.5000e-05\n",
      "Epoch 633/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0894 - accuracy: 0.9820\n",
      "Epoch 00633: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0894 - accuracy: 0.9821 - val_loss: 0.2532 - val_accuracy: 0.9046 - lr: 2.5000e-05\n",
      "Epoch 634/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0905 - accuracy: 0.9797\n",
      "Epoch 00634: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0908 - accuracy: 0.9797 - val_loss: 0.2522 - val_accuracy: 0.9072 - lr: 2.5000e-05\n",
      "Epoch 635/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0903 - accuracy: 0.9809\n",
      "Epoch 00635: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0900 - accuracy: 0.9811 - val_loss: 0.2612 - val_accuracy: 0.9098 - lr: 2.5000e-05\n",
      "Epoch 636/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.9819\n",
      "Epoch 00636: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.0876 - accuracy: 0.9819 - val_loss: 0.2574 - val_accuracy: 0.9072 - lr: 2.5000e-05\n",
      "Epoch 637/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0853 - accuracy: 0.9836\n",
      "Epoch 00637: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 0.0862 - accuracy: 0.9835 - val_loss: 0.2515 - val_accuracy: 0.9162 - lr: 2.5000e-05\n",
      "Epoch 638/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0843 - accuracy: 0.9848\n",
      "Epoch 00638: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0841 - accuracy: 0.9850 - val_loss: 0.2514 - val_accuracy: 0.9059 - lr: 2.5000e-05\n",
      "Epoch 639/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9836\n",
      "Epoch 00639: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0866 - accuracy: 0.9837 - val_loss: 0.2562 - val_accuracy: 0.9098 - lr: 2.5000e-05\n",
      "Epoch 640/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0941 - accuracy: 0.9806\n",
      "Epoch 00640: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0939 - accuracy: 0.9807 - val_loss: 0.2588 - val_accuracy: 0.9111 - lr: 2.5000e-05\n",
      "Epoch 641/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.0891 - accuracy: 0.9812\n",
      "Epoch 00641: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0891 - accuracy: 0.9812 - val_loss: 0.2722 - val_accuracy: 0.8982 - lr: 2.5000e-05\n",
      "Epoch 642/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.0895 - accuracy: 0.9827\n",
      "Epoch 00642: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0895 - accuracy: 0.9827 - val_loss: 0.2578 - val_accuracy: 0.9046 - lr: 2.5000e-05\n",
      "Epoch 643/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0861 - accuracy: 0.9835\n",
      "Epoch 00643: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0863 - accuracy: 0.9834 - val_loss: 0.2528 - val_accuracy: 0.9072 - lr: 2.5000e-05\n",
      "Epoch 644/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0896 - accuracy: 0.9809\n",
      "Epoch 00644: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0892 - accuracy: 0.9811 - val_loss: 0.2574 - val_accuracy: 0.9034 - lr: 2.5000e-05\n",
      "Epoch 645/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0877 - accuracy: 0.9823\n",
      "Epoch 00645: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 40ms/step - loss: 0.0877 - accuracy: 0.9824 - val_loss: 0.2602 - val_accuracy: 0.9046 - lr: 2.5000e-05\n",
      "Epoch 646/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0904 - accuracy: 0.9816\n",
      "Epoch 00646: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0903 - accuracy: 0.9817 - val_loss: 0.2537 - val_accuracy: 0.9124 - lr: 2.5000e-05\n",
      "Epoch 647/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0869 - accuracy: 0.9841\n",
      "Epoch 00647: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0870 - accuracy: 0.9839 - val_loss: 0.2530 - val_accuracy: 0.9072 - lr: 2.5000e-05\n",
      "Epoch 648/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0825 - accuracy: 0.9839\n",
      "Epoch 00648: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0824 - accuracy: 0.9839 - val_loss: 0.2553 - val_accuracy: 0.9085 - lr: 2.5000e-05\n",
      "Epoch 649/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0881 - accuracy: 0.9832\n",
      "Epoch 00649: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0877 - accuracy: 0.9834 - val_loss: 0.2552 - val_accuracy: 0.9046 - lr: 2.5000e-05\n",
      "Epoch 650/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0833 - accuracy: 0.9845\n",
      "Epoch 00650: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 41ms/step - loss: 0.0836 - accuracy: 0.9845 - val_loss: 0.2620 - val_accuracy: 0.9034 - lr: 2.5000e-05\n",
      "Epoch 651/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0875 - accuracy: 0.9828\n",
      "Epoch 00651: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0872 - accuracy: 0.9829 - val_loss: 0.2638 - val_accuracy: 0.9046 - lr: 2.5000e-05\n",
      "Epoch 652/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0901 - accuracy: 0.9794\n",
      "Epoch 00652: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0899 - accuracy: 0.9795 - val_loss: 0.2609 - val_accuracy: 0.9059 - lr: 2.5000e-05\n",
      "Epoch 653/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0893 - accuracy: 0.9820\n",
      "Epoch 00653: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0893 - accuracy: 0.9821 - val_loss: 0.2599 - val_accuracy: 0.9021 - lr: 2.5000e-05\n",
      "Epoch 654/4000\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.0892 - accuracy: 0.9825\n",
      "Epoch 00654: val_loss did not improve from 0.24604\n",
      "70/70 [==============================] - 3s 38ms/step - loss: 0.0892 - accuracy: 0.9825 - val_loss: 0.2605 - val_accuracy: 0.9072 - lr: 2.5000e-05\n",
      "Epoch 655/4000\n",
      "69/70 [============================>.] - ETA: 0s - loss: 0.0789 - accuracy: 0.9871\n",
      "Epoch 00655: val_loss did not improve from 0.24604\n",
      "\n",
      "Epoch 00655: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "70/70 [==============================] - 3s 39ms/step - loss: 0.0793 - accuracy: 0.9870 - val_loss: 0.2541 - val_accuracy: 0.9098 - lr: 2.5000e-05\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "model, history = train_model(train_data,train_labels,val_data,val_labels,'edit18_PC_6_model_n', path='./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_train_history(history ,'accuracy','val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_train_history(history ,'loss','val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "activation=tensorflow.keras.layers.LeakyReLU()\n",
    "model = load_model('edit18_PC_6_model_n_best_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9097938144329897,\n",
       " 'precision': 0.9162303664921466,\n",
       " 'sensitivity': 0.9020618556701031,\n",
       " 'specificity': 0.9175257731958762,\n",
       " 'f1': 0.9090909090909092,\n",
       " 'mcc': 0.8196856415646934}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate\n",
    "metric_array(val_data, val_labels, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load testing data\n",
    "test_AVP_data = PC_6('../data/testing_6db_filter2012out_1_0.fasta', length=50)\n",
    "test_non_AVP_data = PC_6('../data/neg_testing_uniprot_random_combined_293.fasta', length=50)\n",
    "test_AVP_array= np.array(list(test_AVP_data.values()))\n",
    "test_non_AVP_array = np.array(list(test_non_AVP_data.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature & labels\n",
    "test_features = np.concatenate((test_non_AVP_array,test_AVP_array),axis=0)\n",
    "test_labels = np.hstack((np.repeat(0, len(test_non_AVP_array)),np.repeat(1, len(test_AVP_array))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8873720136518771,\n",
       " 'precision': 0.8429003021148036,\n",
       " 'sensitivity': 0.9522184300341296,\n",
       " 'specificity': 0.8225255972696246,\n",
       " 'f1': 0.8942307692307692,\n",
       " 'mcc': 0.7813430798766071}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing\n",
    "metric_array(test_features, test_labels, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
